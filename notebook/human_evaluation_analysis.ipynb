{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62d4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from scipy.stats import kendalltau\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d587b13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    \"target\",\n",
    "    \"fid_abstract\",\n",
    "    #\"led_abstract\",\n",
    "    \"fid_context\",\n",
    "    #\"led_context\",\n",
    "    \"fid_oracle\",\n",
    "    #\"led_oracle\",\n",
    "    \"fid_keyword\",\n",
    "    #\"led_keyword\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d01b6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    \"fluency\", \"coherence\", \"relevance\", \"overall\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e6109c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_config = {}\n",
    "with open(\"group1_configs.json\") as f:\n",
    "    group_config_idx = json.load(f)\n",
    "\n",
    "for ID, indices in group_config_idx.items():\n",
    "    group_config[ID] = [configs[idx] for idx in indices]\n",
    "    \n",
    "with open(\"group2_configs.json\") as f:\n",
    "    group_config_idx = json.load(f)\n",
    "\n",
    "for ID, indices in group_config_idx.items():\n",
    "    group_config[ID] = [configs[idx] for idx in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6084bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_names = [\"Ximeng\",\"Lily\", \"Eric\"]\n",
    "group1_file_names = [\"Ximeng.csv\",\"Lily.csv\", \"EricNgo.csv\"]\n",
    "#group1_names = [\"Ximeng\",\"Lily\"]\n",
    "#group1_file_names = [\"Ximeng.csv\",\"Lily.csv\"]\n",
    "group2_names = [\"Gerardo\",\"Biswadip\",\"Tanmay\"]\n",
    "group2_file_names = [\"gerardo.csv\",\"biswadip.csv\", \"TanmayVakare.csv\"]\n",
    "#group2_names = [\"Biswadip\"]\n",
    "#group2_file_names = [\"biswadip.csv\"]\n",
    "group_names = [group1_names, group2_names]\n",
    "group_file_names = [group1_file_names, group2_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aff6697d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "responses = {k: {config: {metric: {} for metric in metrics} for config in configs} for k in group_config.keys()}\n",
    "for group_name, group_file_name in zip(group_names, group_file_names):\n",
    "    for name, file_name in zip(group_name, group_file_name):\n",
    "        with open(file_name) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                elements = line.strip().split(\",\")\n",
    "                if len(elements)!=6:\n",
    "                    continue\n",
    "                ID, system, fluency, coherence, relevance, overall = elements\n",
    "                if i>0 and ID != \"\":\n",
    "                    system_name = group_config[ID][int(system)-1]\n",
    "                    responses[ID][system_name][\"fluency\"][name] = int(fluency)\n",
    "                    responses[ID][system_name][\"coherence\"][name] = int(coherence)\n",
    "                    responses[ID][system_name][\"relevance\"][name] = int(relevance)\n",
    "                    responses[ID][system_name][\"overall\"][name] = int(overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d772322",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1_target_file_names = [\"Ximeng_target.csv\",\"lily_target.csv\", \"EricNgo_target.csv\"]\n",
    "group2_target_file_names = [\"gerardo_target.csv\", \"biswadip_target.csv\", \"TanmayVakare_target.csv\"]\n",
    "group_names = [group1_names, group2_names]\n",
    "group_target_file_names = [group1_target_file_names, group2_target_file_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c95232e",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_target = {k: {config: {metric: {} for metric in metrics} for config in configs} for k in group_config.keys()}\n",
    "\n",
    "for group_name, group_file_name in zip(group_names, group_target_file_names):\n",
    "    for name, file_name in zip(group_name, group_file_name):\n",
    "        with open(file_name) as f:\n",
    "            for i, line in enumerate(f):\n",
    "                elements = line.strip().split(\",\")\n",
    "                if len(elements)!=6:\n",
    "                    continue\n",
    "                ID, system, fluency, coherence, relevance, overall = elements\n",
    "                \"\"\"\n",
    "                if fluency == \"\":\n",
    "                    fluency = \"5\"\n",
    "                if coherence == \"\":\n",
    "                    coherence = \"5\"\n",
    "                if relevance == \"\":\n",
    "                    relevance = \"5\"\n",
    "                if overall == \"\":\n",
    "                    overall = \"5\"\n",
    "                \"\"\"\n",
    "                if i>0 and ID != \"\":\n",
    "                    if ID[-1] == \"$\":\n",
    "                        ID = ID[:-1]\n",
    "                    system_name = group_config[ID][int(system)-1]\n",
    "                    responses_target[ID][system_name][\"fluency\"][name] = int(fluency)\n",
    "                    responses_target[ID][system_name][\"coherence\"][name] = int(coherence)\n",
    "                    responses_target[ID][system_name][\"relevance\"][name] = int(relevance)\n",
    "                    responses_target[ID][system_name][\"overall\"][name] = int(overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9901121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_agreement(name1, name2):\n",
    "    blind_scores1 = []\n",
    "    blind_scores2 = []\n",
    "    for config in configs:\n",
    "        for metric in metrics:\n",
    "            this_metric = []\n",
    "            for ID in group_config.keys():\n",
    "                if name1 in responses[ID][config][metric]:\n",
    "                    score = responses[ID][config][metric][name1]\n",
    "                    if score > 0:\n",
    "                        blind_scores1.append(score)\n",
    "                if name2 in responses[ID][config][metric]:\n",
    "                    score= responses[ID][config][metric][name2]\n",
    "                    if score > 0:\n",
    "                        blind_scores2.append(score)\n",
    "\n",
    "    target_scores1 = []\n",
    "    target_scores2 = []\n",
    "    for config in configs:\n",
    "        for metric in metrics:\n",
    "            this_metric = []\n",
    "            for ID in group_config.keys():\n",
    "                if name1 in responses_target[ID][config][metric]:\n",
    "                    score = responses_target[ID][config][metric][name1]\n",
    "                    if score > 0:\n",
    "                        target_scores1.append(score)\n",
    "                if name2 in responses_target[ID][config][metric]:\n",
    "                    score = responses_target[ID][config][metric][name2]\n",
    "                    if score > 0:\n",
    "                        target_scores2.append(score)\n",
    "\n",
    "    combined_scores1 = blind_scores1 + target_scores1\n",
    "    combined_scores2 = blind_scores2 + target_scores2\n",
    "    return kendalltau(combined_scores1, combined_scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef32707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kendall = {}\n",
    "for group in group_names:\n",
    "    for i in range(len(group)):\n",
    "        for j in range(i+1,len(group)):\n",
    "            name1 = group[i]\n",
    "            name2 = group[j]\n",
    "            kendall = pair_agreement(name1, name2)\n",
    "            all_kendall[name1+\" \"+name2] = kendall.correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ec6fd3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ximeng Lily': 0.20090085672615393,\n",
       " 'Ximeng Eric': 0.15006789504157894,\n",
       " 'Lily Eric': 0.22760719375294797,\n",
       " 'Gerardo Biswadip': 0.2868042008605897,\n",
       " 'Gerardo Tanmay': 0.30872041451010324,\n",
       " 'Biswadip Tanmay': 0.26041109229171566}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_kendall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "031af6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23908527553051492"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(all_kendall.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e9e6296",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = {\"67855635_1_0_2@748227@85501317\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cb3896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_across_examples = {config: {metric: {} for metric in metrics} for config in configs}\n",
    "for config in configs:\n",
    "    for metric in metrics:\n",
    "        this_metric = []\n",
    "        for ID in group_config.keys():\n",
    "            if ID in exclude:\n",
    "                continue\n",
    "            scores = []\n",
    "            for name, score in responses[ID][config][metric].items():\n",
    "                if score > 0:\n",
    "                    scores.append(score)\n",
    "            if len(scores) > 0:\n",
    "                this_example = np.mean(scores)\n",
    "                this_metric.append(this_example)\n",
    "        responses_across_examples[config][metric] = np.round(np.mean(this_metric),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "948c2c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_target_across_examples = {config: {metric: {} for metric in metrics} for config in configs}\n",
    "for config in configs:\n",
    "    for metric in metrics:\n",
    "        this_metric = []\n",
    "        for ID in group_config.keys():\n",
    "            if ID in exclude:\n",
    "                continue\n",
    "            if len(responses_target[ID][config][metric])>0:\n",
    "                scores = []\n",
    "                for name, score in responses_target[ID][config][metric].items():\n",
    "                    if score > 0:\n",
    "                        scores.append(score)\n",
    "                if len(scores) > 0:\n",
    "                    this_example = np.mean(scores)\n",
    "                    this_metric.append(this_example)\n",
    "        responses_target_across_examples[config][metric] = np.round(np.mean(this_metric),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ed3880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': {'fluency': 4.71,\n",
       "  'coherence': 4.23,\n",
       "  'relevance': 4.04,\n",
       "  'overall': 3.87},\n",
       " 'fid_abstract': {'fluency': 4.71,\n",
       "  'coherence': 4.06,\n",
       "  'relevance': 4.29,\n",
       "  'overall': 3.77},\n",
       " 'fid_context': {'fluency': 4.82,\n",
       "  'coherence': 4.0,\n",
       "  'relevance': 4.41,\n",
       "  'overall': 3.95},\n",
       " 'fid_oracle': {'fluency': 4.8,\n",
       "  'coherence': 4.07,\n",
       "  'relevance': 4.18,\n",
       "  'overall': 3.86},\n",
       " 'fid_keyword': {'fluency': 4.84,\n",
       "  'coherence': 4.07,\n",
       "  'relevance': 4.21,\n",
       "  'overall': 3.84}}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_across_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "584867b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': {'fluency': 4.74,\n",
       "  'coherence': 4.41,\n",
       "  'relevance': 4.18,\n",
       "  'overall': 4.16},\n",
       " 'fid_abstract': {'fluency': 4.72,\n",
       "  'coherence': 3.96,\n",
       "  'relevance': 4.26,\n",
       "  'overall': 3.63},\n",
       " 'fid_context': {'fluency': 4.8,\n",
       "  'coherence': 3.96,\n",
       "  'relevance': 4.31,\n",
       "  'overall': 3.85},\n",
       " 'fid_oracle': {'fluency': 4.8,\n",
       "  'coherence': 4.09,\n",
       "  'relevance': 4.14,\n",
       "  'overall': 3.8},\n",
       " 'fid_keyword': {'fluency': 4.84,\n",
       "  'coherence': 4.07,\n",
       "  'relevance': 4.16,\n",
       "  'overall': 3.76}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses_target_across_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb7dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_responses = {}\n",
    "for ID, results in responses.items():\n",
    "    if ID not in exclude:\n",
    "        this_example = {}\n",
    "        for system, metric_results in results.items():\n",
    "            this_system = {}\n",
    "            for metric, scores in metric_results.items():\n",
    "                this_system[metric] = np.round(np.mean([score for annotator, score in scores.items()]),2)\n",
    "            this_example[system] = this_system\n",
    "        average_responses[ID] = this_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f14c75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "average_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ae9f4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for ID, results in average_responses.items():\n",
    "    minimum = np.min([metric_results[\"overall\"] for system, metric_results in results.items()])\n",
    "    if results[\"fid_oracle\"][\"overall\"] == minimum:\n",
    "        result = {system: metric_results[\"overall\"] for system, metric_results in results.items()}\n",
    "        print(ID)\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2af6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ID, results in average_responses.items():\n",
    "    maximum = np.max([metric_results[\"overall\"] for system, metric_results in results.items()])\n",
    "    if results[\"fid_oracle\"][\"overall\"] == maximum:\n",
    "        result = {system: metric_results[\"overall\"] for system, metric_results in results.items()}\n",
    "        print(ID)\n",
    "        print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8610e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses[\"173990267_1_0_4@5068376\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1e1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_distribution = {config:[] for config in configs}\n",
    "for ID, ratings in average_responses.items():\n",
    "    for config, scores in ratings.items():\n",
    "        score_distribution[config].append(scores[\"overall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3fc656",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,4.5))\n",
    "cfgs = []\n",
    "for config, scores in score_distribution.items():\n",
    "    ax.hist(scores, alpha=0.5)\n",
    "    cfgs.append(config)\n",
    "# Add a legend\n",
    "plt.legend(cfgs, loc='best')\n",
    "\n",
    "#plt.xlabel('# of Sentences per Paper')\n",
    "#plt.ylabel('Fraction')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.xaxis.set_ticks_position('bottom')\n",
    "ax.yaxis.set_ticks_position('left')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d996737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for config, scores in score_distribution.items():\n",
    "    print(config, np.std(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5935756d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for config, scores in score_distribution.items():\n",
    "    fig, ax = plt.subplots(figsize=(6,4.5))\n",
    "    ax.hist(scores, alpha=0.5)\n",
    "    cfgs.append(config)\n",
    "# Add a legend\n",
    "    plt.legend([config], loc='best')\n",
    "\n",
    "    #plt.xlabel('# of Sentences per Paper')\n",
    "    #plt.ylabel('Fraction')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14887aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
