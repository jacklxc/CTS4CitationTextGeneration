{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b965734",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import xml.etree.ElementTree as ET\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dca27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a8b0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(text):\n",
    "    text = text.replace(\"&quot;\",'\"')\n",
    "    text = text.replace(\"-\",' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "095c3dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(sentence):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        if token not in stops:\n",
    "            cleaned_tokens.append(token)\n",
    "    return \" \".join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bcbc316",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ed770b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1','rouge2'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49db3264",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/home/xxl190027/scisumm-corpus/data/Test-Set-2018/\"\n",
    "files = glob(base_dir+\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f023958c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_citing_paper_sentences = {}\n",
    "all_citing_paper_sentences_lookup = {}\n",
    "for file in files:\n",
    "    citing_files = glob(file+\"/Citance_XML/*.xml\")\n",
    "    for xml_path in citing_files:\n",
    "        paper_name = os.path.split(xml_path)[-1].split(\".\")[0]\n",
    "        tree = ET.parse(xml_path)\n",
    "        root = tree.getroot()\n",
    "        sentences = {}\n",
    "        sentences_lookup = {}\n",
    "        for node in root.findall(\"S\"):\n",
    "            sentence = node.text.strip().replace(\"  \",\" \")\n",
    "            sentence = clean_sentence(sentence)\n",
    "            sid = int(node.attrib[\"sid\"])\n",
    "            sentences[sid] = sentence\n",
    "            sentences_lookup[sentence] = sid\n",
    "        for abstract_node in root.findall(\"ABSTRACT\"):\n",
    "            for node in abstract_node.findall(\"S\"):\n",
    "                sentence = node.text.strip().replace(\"  \",\" \")\n",
    "                sentence = clean_sentence(sentence)\n",
    "                sid = int(node.attrib[\"sid\"])\n",
    "                sentences[sid] = sentence\n",
    "                sentences_lookup[sentence] = sid\n",
    "        for section_node in root.findall(\"SECTION\"):\n",
    "            for node in section_node.findall(\"S\"):\n",
    "                sentence = node.text.strip().replace(\"  \",\" \")\n",
    "                sentence = clean_sentence(sentence)\n",
    "                sid = int(node.attrib[\"sid\"])\n",
    "                sentences[sid] = sentence\n",
    "                sentences_lookup[sentence] = sid\n",
    "        all_citing_paper_sentences[paper_name] = sentences\n",
    "        all_citing_paper_sentences_lookup[paper_name] = sentences_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b27bb86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "citing_titles = {citing_paper_name: sentences[0] for citing_paper_name, sentences in all_citing_paper_sentences.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b8b1a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "citing_titles_lookup = {v:k for k,v in citing_titles.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ebc0e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "citing_titles_list = list(citing_titles_lookup.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0d2a2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40317it [00:00, 40379.53it/s]\n"
     ]
    }
   ],
   "source": [
    "acl_title_map = OrderedDict()\n",
    "with open(\"/data/XiangciLi/20200705v1/acl/metadata.jsonl\") as f:\n",
    "    for line in tqdm(f):\n",
    "        meta = json.loads(line)\n",
    "        acl_title_map[meta[\"paper_id\"]] = meta[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f40594ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "acl_title_map_lookup = {v:k for k,v in acl_title_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c6a64c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "distant_ids = []\n",
    "with open(\"sorted_sentence_ROUGE_distant_all.jsonl\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        distant_ids.append(obj[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b172838",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = []\n",
    "with open(\"sorted_sentence_ROUGE_train_all.jsonl\") as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        train_ids.append(obj[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "98ee72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_s2orc_ids = set([k.split(\"_\")[0] for k in (distant_ids + train_ids)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c07d5475",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_s2orc_titles = [acl_title_map[ID] for ID in seen_s2orc_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eb874bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Injecting Logical Background Knowledge into Embeddings for Relation Extraction',\n",
       " 'Creating a Corpus for Russian Data-to-Text Generation Using Neural Machine Translation and Post-Editing',\n",
       " 'Transformer-based Neural Machine Translation System for Tamil – English',\n",
       " 'Combining Knowledge Hunting and Neural Language Models to Solve the Winograd Schema Challenge',\n",
       " 'The Arabic Online Commentary Dataset: an Annotated Dataset of Informal Arabic with High Dialectal Content',\n",
       " 'MalwareTextDB: A Database for Annotated Malware Articles',\n",
       " 'Multi-Task Learning for Coherence Modeling',\n",
       " 'context2vec: Learning Generic Context Embedding with Bidirectional LSTM',\n",
       " 'The Role of Context in Neural Morphological Disambiguation',\n",
       " 'The price of debiasing automatic metrics in natural language evaluation',\n",
       " 'Stance Classification in Rumours as a Sequential Task Exploiting the Tree Structure of Social Media Conversations',\n",
       " 'Obligation and Prohibition Extraction Using Hierarchical RNNs',\n",
       " 'Aligning Features With Sense Distinction Dimensions',\n",
       " 'Joint Part-of-Speech and Language ID Tagging for Code-Switched Data',\n",
       " 'Neural Semantic Encoders',\n",
       " 'Linking the Thoughts: Analysis of Argumentation Structures in Scientific Publications',\n",
       " 'Stop-probability estimates computed on a large corpus improve Unsupervised Dependency Parsing',\n",
       " 'Reinforced Training Data Selection for Domain Adaptation',\n",
       " 'Hybrid Adaptation of Named Entity Recognition for Statistical Machine Translation',\n",
       " 'Multimodal Affective Analysis Using Hierarchical Attention Strategy with Word-Level Alignment',\n",
       " 'Group, Extract and Aggregate: Summarizing a Large Amount of Finance News for Forex Movement Prediction',\n",
       " 'Embedding Syntax and Semantics of Prepositions via Tensor Decomposition',\n",
       " 'Supervised Sentence Fusion with Single-Stage Inference',\n",
       " 'Solving Feature Sparseness in Text Classification using Core-Periphery Decomposition',\n",
       " 'Automatic Mapping of French Discourse Connectives to PDTB Discourse Relations',\n",
       " 'Learning Word Importance with the Neural Bag-of-Words Model',\n",
       " 'Detecting Semantically Equivalent Questions in Online User Forums',\n",
       " 'Learning Scalar Adjective Intensity from Paraphrases',\n",
       " 'Response-based Learning for Machine Translation of Open-domain Database Queries',\n",
       " 'Dependency Parsing for Weibo: An Efficient Probabilistic Logic Programming Approach',\n",
       " 'TKLBLIIR: Detecting Twitter Paraphrases with TweetingJay',\n",
       " 'Rethinking Skip-thought: A Neighborhood based Approach',\n",
       " 'WSLLN: Weakly Supervised Natural Language Localization Networks',\n",
       " 'Improved Temporal Relation Classification using Dependency Parses and Selective Crowdsourced Annotations',\n",
       " 'On the Correspondence between Compositional Matrix-Space Models of Language and Weighted Automata',\n",
       " 'Locally Non-Linear Learning for Statistical Machine Translation via Discretization and Structured Regularization',\n",
       " 'Investigations on Word Senses and Word Usages',\n",
       " 'A Hybrid Stepwise Approach for De-identifying Person Names in Clinical Documents',\n",
       " 'A Dependency Structure Annotation for Modality',\n",
       " 'HHU at SemEval-2016 Task 1: Multiple Approaches to Measuring Semantic Textual Similarity',\n",
       " 'Automatic Extraction of News Values from Headline Text',\n",
       " \"Learning What's Easy: Fully Differentiable Neural Easy-First Taggers\",\n",
       " 'Multi-Source Domain Adaptation with Mixture of Experts',\n",
       " 'Exploiting Multiple Treebanks for Parsing with Quasi-synchronous Grammars',\n",
       " 'Not Just Depressed: Bipolar Disorder Prediction on Reddit',\n",
       " 'Edlin: an Easy to Read Linear Learning Framework',\n",
       " 'HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization',\n",
       " 'What You Say and How You Say it: Joint Modeling of Topics and Discourse in Microblog Conversations',\n",
       " 'Learning Tractable Word Alignment Models with Complex Constraints',\n",
       " 'Interpretation of Implicit Conditions in Database Search Dialogues',\n",
       " 'Curate and Generate: A Corpus and Method for Joint Control of Semantics and Style in Neural NLG',\n",
       " 'A Generative Parser with a Discriminative Recognition Algorithm',\n",
       " 'Learning Script Knowledge with Web Experiments',\n",
       " 'Bornholmsk Natural Language Processing: Resources and Tools.',\n",
       " 'Sparse Bilingual Word Representations for Cross-lingual Lexical Entailment',\n",
       " 'Deception detection in Russian texts.',\n",
       " 'Sentence Packaging in Text Generation from Semantic Graphs as a Community Detection Problem',\n",
       " 'From Text to Lexicon: Bridging the Gap between Word Embeddings and Lexical Resources',\n",
       " 'Microblogs as Parallel Corpora',\n",
       " 'A Hybrid Statistical Approach for Named Entity Recognition for Malayalam Language',\n",
       " 'Automatic Slide Generation Based on Discourse Structure Analysis',\n",
       " 'Multilingual Code-switching Identification via LSTM Recurrent Neural Networks',\n",
       " 'Revising the Compositional Method for Terminology Acquisition from Comparable Corpora',\n",
       " 'Incorporating Knowledge Resources to Enhance Medical Information Extraction',\n",
       " 'Modeling Conversation Structure and Temporal Dynamics for Jointly Predicting Rumor Stance and Veracity',\n",
       " 'CROWD-IN-THE-LOOP: A Hybrid Approach for Annotating Semantic Roles',\n",
       " 'Special Techniques for Constituent Parsing of Morphologically Rich Languages',\n",
       " 'EmotionX-JTML: Detecting emotions with Attention',\n",
       " \"Predicting a Scientific Community's Response to an Article\",\n",
       " 'Deriving a Lexicon for a Precision Grammar from Language Documentation Resources: A Case Study of Chintang',\n",
       " 'Making Readability Indices Readable',\n",
       " 'Fine-Tuning Improves Claim Detection',\n",
       " 'Some Experiments in Mining Named Entity Transliteration Pairs from Comparable Corpora',\n",
       " 'Iteratively Estimating Pattern Reliability and Seed Quality With Extraction Consistency *',\n",
       " 'Extracting Drug-Drug Interactions with Attention CNNs',\n",
       " 'EHU at the SIGMORPHON 2016 Shared Task. A Simple Proposal: Grapheme-to-Phoneme for Inflection',\n",
       " 'Additive Compositionality of Word Vectors.',\n",
       " 'Towards Qualitative Word Embeddings Evaluation: Measuring Neighbors Variation',\n",
       " 'Exploring the Use of Word Relation Features for Sentiment Classification',\n",
       " 'Deep Reinforcement Learning with a Combinatorial Action Space for Predicting Popular Reddit Threads',\n",
       " 'Unsupervised Pivot Translation for Distant Languages',\n",
       " 'Extracting Paraphrases From A Parallel Corpus',\n",
       " 'AutoSeM: Automatic Task Selection and Mixing in Multi-Task Learning',\n",
       " 'Talking to the crowd: What do people react to in online discussions?',\n",
       " 'Modeling Relation Paths for Representation Learning of Knowledge Bases',\n",
       " 'Towards an on-demand Simple Portuguese Wikipedia',\n",
       " 'Entity-Centric Joint Modeling of Japanese Coreference Resolution and Predicate Argument Structure Analysis',\n",
       " 'Large-Scale Verb Entailment Acquisition from the Web',\n",
       " 'Improving neural knowledge base completion with cross-lingual projections',\n",
       " 'A Discursive Grid Approach to Model Local Coherence in Multi-document Summaries',\n",
       " 'Spider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-SQL Task',\n",
       " 'Bilingual Word Representations with Monolingual Quality in Mind',\n",
       " 'Learning to Link Grammar and Encyclopedic Information of Assist ESL Learners',\n",
       " 'A Sequencing Model for Situation Entity Classification',\n",
       " 'An Empirical Assessment of Contemporary Online Media in Ad-Hoc Corpus Creation for Social Events',\n",
       " 'Semantic Class Induction and Coreference Resolution',\n",
       " 'Sarcasm Detection on Czech and English Twitter',\n",
       " 'Improving Event Detection with Abstract Meaning Representation',\n",
       " 'Pairwise Word Interaction Modeling with Deep Neural Networks for Semantic Similarity Measurement',\n",
       " 'Jointly Learning Word Representations and Composition Functions Using Predicate-Argument Structures',\n",
       " 'Learning Interpretable Negation Rules via Weak Supervision at Document Level: A Reinforcement Learning Approach',\n",
       " 'Sentence-level Rewriting Detection',\n",
       " 'Multi-Document Summarization using Automatic Key-Phrase Extraction',\n",
       " 'Multi-glance Reading Model for Text Understanding',\n",
       " 'Explain Yourself! Leveraging Language Models for Commonsense Reasoning',\n",
       " 'Learning Embeddings to lexicalise RDF Properties',\n",
       " 'The Good, the Bad, and the Unknown: Morphosyllabic Sentiment Tagging of Unseen Words',\n",
       " 'Latent Domain Word Alignment for Heterogeneous Corpora',\n",
       " 'Automatic Extraction of Morphological Lexicons from Morphologically Annotated Corpora',\n",
       " 'A Joint Model of Implicit Arguments for Nominal Predicates',\n",
       " 'NLPSA at IJCNLP-2017 Task 2: Imagine Scenario: Leveraging Supportive Images for Dimensional Sentiment Analysis',\n",
       " 'Towards Syntactic Iberian Polarity Classification',\n",
       " 'Retrieval of Research-level Mathematical Information Needs: A Test Collection and Technical Terminology Experiment',\n",
       " 'Collective Classification for Fine-grained Information Status',\n",
       " 'Inducing a Lexicon of Abusive Words – a Feature-Based Approach',\n",
       " 'Clustering Paraphrases by Word Sense',\n",
       " 'Learning from Chunk-based Feedback in Neural Machine Translation',\n",
       " 'Tell-and-Answer: Towards Explainable Visual Question Answering using Attributes and Captions',\n",
       " 'Fermi at SemEval-2019 Task 4 : The sarah-jane-smith Hyperpartisan News Detector',\n",
       " 'Incorporating Subword Information into Matrix Factorization Word Embeddings',\n",
       " 'A Language Independent Approach for Name Categorization and Discrimination',\n",
       " 'A Joint Graph Model for Pinyin-to-Chinese Conversion with Typo Correction',\n",
       " 'Translating Phrases in Neural Machine Translation',\n",
       " 'WriteAhead: Mining Grammar Patterns in Corpora for Assisted Writing',\n",
       " 'Language Technology for Agile Social Media Science',\n",
       " 'The UWNLP system at SemEval-2018 Task 7: Neural Relation Extraction Model with Selectively Incorporated Concept Embeddings',\n",
       " 'How Many Words Is a Picture Worth? Automatic Caption Generation for News Images',\n",
       " 'Weakly Supervised User Profile Extraction from Twitter',\n",
       " 'Exact Decoding of Phrase-Based Translation Models through Lagrangian Relaxation',\n",
       " 'Statistical Shallow Semantic Parsing Despite Little Training Data',\n",
       " 'Toward a Comparable Corpus of Latvian, Russian and English Tweets.',\n",
       " 'Truth of Varying Shades: Analyzing Language in Fake News and Political Fact-Checking',\n",
       " 'Joint Embedding of Query and Ad by Leveraging Implicit Feedback',\n",
       " 'RUBISC - a Robust Unification-Based Incremental Semantic Chunker',\n",
       " 'Neural Duplicate Question Detection without Labeled Training Data',\n",
       " 'Foreign Words and the Automatic Processing of Arabic Social Media Text Written in Roman Script',\n",
       " 'A Generalized Vector Space Model for Text Retrieval Based on Semantic Relatedness',\n",
       " 'Computational Ad Hominem Detection',\n",
       " 'Concept-Map-Based Multi-Document Summarization using Concept Coreference Resolution and Global Importance Optimization',\n",
       " 'Tweet Recommendation with Graph Co-Ranking',\n",
       " 'RNN Embeddings for Identifying Difficult to Understand Medical Words',\n",
       " 'Identifying Event-related Bursts via Social Media Activities',\n",
       " 'Multilingual Grammar Induction with Continuous Language Identification',\n",
       " 'Predicting Success in Dialogue',\n",
       " 'A Question Answering Approach to Emotion Cause Extraction',\n",
       " 'Constrained Grammatical Error Correction using Statistical Machine Translation',\n",
       " 'Understanding the Polarity of Events in the Biomedical Literature: Deep Learning vs. Linguistically-informed Methods',\n",
       " 'Person Cross Document Coreference with Name Perplexity Estimates',\n",
       " 'Cache-Augmented Latent Topic Language Models for Speech Retrieval',\n",
       " 'Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Path',\n",
       " 'Robust Subgraph Generation Improves Abstract Meaning Representation Parsing',\n",
       " \"Don't understand a measure? Learn it: Structured Prediction for Coreference Resolution optimizing its measures\",\n",
       " 'Language Independent Text Correction using Finite State Automata',\n",
       " 'Together We Can: Bilingual Bootstrapping for WSD',\n",
       " 'Towards Contextual Healthiness Classification of Food Items - A Linguistic Approach',\n",
       " 'Lexical Tightness and Text Complexity',\n",
       " 'Knowledge Base Inference using Bridging Entities',\n",
       " 'Models and Training for Unsupervised Preposition Sense Disambiguation',\n",
       " 'Tagging The Web: Building A Robust Web Tagger with Neural Network',\n",
       " 'CFO: Conditional Focused Neural Question Answering with Large-scale Knowledge Bases',\n",
       " 'Language-Independent Discriminative Parsing of Temporal Expressions',\n",
       " 'A Concept-Centered Approach to Noun-Compound Interpretation',\n",
       " 'Relieving The Data Acquisition Bottleneck In Word Sense Disambiguation',\n",
       " 'A Report on the 2018 VUA Metaphor Detection Shared Task',\n",
       " 'Tailored Sequence to Sequence Models to Different Conversation Scenarios',\n",
       " 'A low-budget tagger for Old Czech',\n",
       " 'Promoting multiword expressions in A* TAG parsing',\n",
       " 'The FrameNet Data And Software',\n",
       " 'An Unsupervised Ranking Model for Noun-Noun Compositionality',\n",
       " 'Parse, Price and Cut–-Delayed Column and Row Generation for Graph Based Parsers',\n",
       " 'Concept Grounding to Multiple Knowledge Bases via Indirect Supervision',\n",
       " \"Discriminating Among Word Senses Using McQuitty's Similarity Analysis\",\n",
       " 'A first order semantic approach to adjectival inference',\n",
       " 'Measuring Term Informativeness in Context',\n",
       " 'Empirical comparison of dependency conversions for RST discourse trees',\n",
       " 'Automatically Identifying Implicit Arguments to Improve Argument Linking and Coherence Modeling',\n",
       " 'Joint Word Segmentation and Phonetic Category Induction',\n",
       " 'Convolution Kernels on Constituent, Dependency and Sequential Structures for Relation Extraction',\n",
       " 'Question Answering based on Semantic Roles',\n",
       " 'Crowd-Sourced Iterative Annotation for Narrative Summarization Corpora.',\n",
       " 'Correction Annotation for Non-Native Arabic Texts: Guidelines and Corpus',\n",
       " 'Guiding the Flowing of Semantics: Interpretable Video Captioning via POS Tag',\n",
       " 'Binarized LSTM Language Model',\n",
       " 'Topic Modeling based Sentiment Analysis on Social Media for Stock Market Prediction',\n",
       " 'AFET: Automatic Fine-Grained Entity Typing by Hierarchical Partial-Label Embedding',\n",
       " 'IWNLP: Inverse Wiktionary for Natural Language Processing',\n",
       " 'Storyline detection and tracking using Dynamic Latent Dirichlet Allocation',\n",
       " 'Using Large Monolingual and Bilingual Corpora to Improve Coordination Disambiguation',\n",
       " 'Siamese CBOW: Optimizing Word Embeddings for Sentence Representations',\n",
       " 'Stance Classification on PTT Comments',\n",
       " 'Automatic Learning Of Language Model Structure',\n",
       " 'Identification of Social Acts in Dialogue',\n",
       " 'Open Dialogue Management for Relational Databases',\n",
       " 'Long-Tail Distributions and Unsupervised Learning of Morphology',\n",
       " 'Structural Embedding of Syntactic Trees for Machine Comprehension',\n",
       " 'Paraphrase Detection for Short Answer Scoring',\n",
       " 'Improved Unsupervised POS Induction through Prototype Discovery',\n",
       " 'Semi-supervised Learning for Vietnamese Named Entity Recognition using Online Conditional Random Fields',\n",
       " 'Modeling Past and Future for Neural Machine Translation',\n",
       " 'Clustering of Russian Adjective-Noun Constructions using Word Embeddings',\n",
       " 'WSD for n-best reranking and local language modeling in SMT',\n",
       " 'Automatic Transliteration of Romanized Dialectal Arabic',\n",
       " 'Where Not to Eat? Improving Public Policy by Predicting Hygiene Inspections Using Online Reviews',\n",
       " 'CogALex-V Shared Task: CGSRC - Classifying Semantic Relations using Convolutional Neural Networks.',\n",
       " 'Evaluating the Word Sense Disambiguation Performance of Statistical Machine Translation',\n",
       " 'Estimating Semantic Distance Using Soft Semantic Constraints in Knowledge-Source – Corpus Hybrid Models',\n",
       " 'Temporal Centering',\n",
       " 'Combining resources for MWE-token classification',\n",
       " 'Sequence-to-Dependency Neural Machine Translation',\n",
       " 'Detection of Multiword Expressions for Hindi Language using Word Embeddings and WordNet-based Features',\n",
       " 'Hey Siri. Ok Google. Alexa: A topic modeling of user reviews for smart speakers.',\n",
       " 'Cancer Stage Prediction Based on Patient Online Discourse',\n",
       " 'Dataset for a Neural Natural Language Interface for Databases (NNLIDB)',\n",
       " 'The Effect of Author Set Size in Authorship Attribution for Lithuanian',\n",
       " 'ARHNet - Leveraging Community Interaction for Detection of Religious Hate Speech in Arabic',\n",
       " 'Likey: Unsupervised Language-Independent Keyphrase Extraction',\n",
       " 'Design of an Active Learning System with Human Correction for Content Analysis',\n",
       " 'Neural Networks Leverage Corpus-wide Information for Part-of-speech Tagging',\n",
       " 'Sentiment Intensity Ranking among Adjectives Using Sentiment Bearing Word Embeddings',\n",
       " 'Partial Parsing: A Report On Work In Progress',\n",
       " 'Grammar Engineering for a Customer: a Case Study with Five Languages',\n",
       " 'Compositional Sequence Labeling Models for Error Detection in Learner Writing',\n",
       " 'A Framework for Learning Morphology using Suffix Association Matrix',\n",
       " 'Multi- and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception',\n",
       " 'Incremental Construction of Robust but Deep Semantic Representations for Use in Responsive Dialogue Systems',\n",
       " 'Filtering Antonymous, Trend-Contrasting, and Polarity-Dissimilar Distributional Paraphrases for Improving Statistical Machine Translation',\n",
       " 'Arabic Dialect Identification in Speech Transcripts.',\n",
       " 'Bootstrapped Training of Event Extraction Classifiers',\n",
       " 'An Efficient Active Learning Framework for New Relation Types',\n",
       " 'Towards Incremental Learning of Word Embeddings Using Context Informativeness',\n",
       " 'Machine Translation of Spanish Personal and Possessive Pronouns Using Anaphora Probabilities',\n",
       " 'Textual Demand Analysis: Detection of Users% Wants and Needs from Opinions',\n",
       " 'Linguistically Annotated BTG for Statistical Machine Translation',\n",
       " 'A modular open-source focused crawler for mining monolingual and bilingual corpora from the web',\n",
       " 'Spontaneous Speech Understanding For Robust Multi-Modal Human-Robot Communication',\n",
       " 'Robust Dictionary Lookup in Multiple Noisy Orthographies',\n",
       " 'Dialogue Act Classification with Context-Aware Self-Attention',\n",
       " 'Question Relevance in VQA: Identifying Non-Visual And False-Premise Questions',\n",
       " 'Participant Subjectivity and Involvement as a Basis for Discourse Segmentation',\n",
       " 'Learning Effective Surface Text Patterns For Information Extraction',\n",
       " 'Weakly Supervised Role Identification in Teamwork Interactions',\n",
       " 'Weakly supervised learning of allomorphy',\n",
       " 'Towards Spanish Verbs’ Selectional Preferences Automatic Acquisition: Semantic Annotation of the SenSem Corpus',\n",
       " 'On the Problem of Theoretical Terms in Empirical Computational Linguistics',\n",
       " 'Does more data always yield better translations?',\n",
       " 'A Neural Model for Language Identification in Code-Switched Tweets',\n",
       " 'Read and Comprehend by Gated-Attention Reader with More Belief',\n",
       " 'Label Embedding for Zero-shot Fine-grained Named Entity Typing',\n",
       " 'Spatial Aggregation Facilitates Discovery of Spatial Topics',\n",
       " 'Hybrid Neural Network Alignment and Lexicon Model in Direct HMM for Statistical Machine Translation',\n",
       " 'Empirical Exploitation of Click Data for Task Specific Ranking',\n",
       " 'Word-level Language Identification in Bi-lingual Code-switched Texts',\n",
       " 'Ensemble Semantics for Large-scale Unsupervised Relation Extraction',\n",
       " 'High-Accuracy Phrase Translation Acquisition Through Battle-Royale Selection',\n",
       " 'Passive diagnosis incorporating the PHQ-4 for depression and anxiety',\n",
       " 'Translation Acquisition Using Synonym Sets',\n",
       " 'Context-Aware Representations for Knowledge Base Relation Extraction',\n",
       " 'Efficient retrieval of tree translation examples for Syntax-Based Machine Translation',\n",
       " 'A statistical approach for Non-Sentential Utterance Resolution for Interactive QA System',\n",
       " 'Mining Cross-Cultural Differences and Similarities in Social Media',\n",
       " 'IISCNLP at SemEval-2016 Task 2: Interpretable STS with ILP based Multiple Chunk Aligner',\n",
       " 'Pushing the Limits of Low-Resource Morphological Inflection',\n",
       " 'How NOT To Evaluate Your Dialogue System: An Empirical Study of Unsupervised Evaluation Metrics for Dialogue Response Generation',\n",
       " 'Domain-Adaptable Hybrid Generation of RDF Entity Descriptions',\n",
       " 'Modeling Mobile Intention Recognition Problems with Spatially Constrained Tree-Adjoining Grammars',\n",
       " 'Accelerating Asynchronous Stochastic Gradient Descent for Neural Machine Translation',\n",
       " 'Learning Adaptable Patterns for Passage Reranking',\n",
       " 'Unsupervised Learning of PCFGs with Normalizing Flow',\n",
       " 'Sentiment Classification Considering Negation and Contrast Transition',\n",
       " 'Language Independent Sentiment Analysis with Sentiment-Specific Word Embeddings',\n",
       " 'Sampling Bias in Deep Active Classification: An Empirical Study',\n",
       " 'Unsupervised Grammar Induction with Depth-bounded PCFG',\n",
       " 'Learning Subjective Nouns Using Extraction Pattern Bootstrapping',\n",
       " 'Using Argument Mining to Assess the Argumentation Quality of Essays',\n",
       " 'Characterizing the Language of Online Communities and its Relation to Community Reception',\n",
       " 'Applying Morphology Generation Models to Machine Translation',\n",
       " 'Discriminative Lexicon Adaptation for Improved Character Accuracy - A New Direction in Chinese Language Modeling',\n",
       " 'Attribute Extraction from Synthetic Web Search Queries',\n",
       " 'Syntax Helps ELMo Understand Semantics: Is Syntax Still Relevant in a Deep Neural Architecture for SRL?',\n",
       " 'Low-Resource Syntactic Transfer with Unsupervised Source Reordering',\n",
       " 'Annotating Discourse Anaphora',\n",
       " 'When Did that Happen? — Linking Events and Relations to Timestamps',\n",
       " 'Hierarchical Neural Story Generation',\n",
       " 'Adopting the Word-Pair-Dependency-Triplets with Individual Comparison for Natural Language Inference',\n",
       " 'Using Context to Predict the Purpose of Argumentative Writing Revisions',\n",
       " 'Learning Concept Abstractness Using Weak Supervision',\n",
       " 'Can Click Patterns across User’s Query Logs Predict Answers to Definition Questions?',\n",
       " 'Dependency-Based Semantic Role Labeling using Convolutional Neural Networks',\n",
       " 'Learning from Explicit and Implicit Supervision Jointly For Algebra Word Problems',\n",
       " 'Identification, characterization, and grounding of gradable terms in clinical text',\n",
       " 'A Description of Tunable Machine Translation Evaluation Systems in WMT13 Metrics Task',\n",
       " 'Turkish Treebanking: Unifying and Constructing Efforts',\n",
       " 'CogALex-V Shared Task: GHHH - Detecting Semantic Relations via Word Embeddings',\n",
       " 'Measuring Popularity of Machine-Generated Sentences Using Term Count, Document Frequency, and Dependency Language Model',\n",
       " 'A Graph-Theoretic Algorithm for Automatic Extension of Translation Lexicons',\n",
       " 'Do Enterprises Have Emotions?',\n",
       " 'Annotating Spelling Errors in German Texts Produced by Primary School Children',\n",
       " 'Dynamic Past and Future for Neural Machine Translation',\n",
       " 'SemEval-2012 Task 3: Spatial Role Labeling',\n",
       " 'Reinforcement-based denoising of distantly supervised NER with partial annotation',\n",
       " 'Zero-shot Learning of Classifiers from Natural Language Quantification',\n",
       " 'PubMedQA: A Dataset for Biomedical Research Question Answering',\n",
       " 'Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging',\n",
       " 'Improving Low-Resource Neural Machine Translation with Filtered Pseudo-Parallel Corpus',\n",
       " 'Sprinkling Topics for Weakly Supervised Text Classification',\n",
       " 'Combining Contexts in Lexicon Learning for Semantic Parsing',\n",
       " 'Entity Clustering Across Languages',\n",
       " 'Improving Coreference Resolution by Using Conversational Metadata',\n",
       " 'Investigating the Contribution of Distributional Semantic Information for Dialogue Act Classification',\n",
       " 'Temporal Text Ranking and Automatic Dating of Texts',\n",
       " 'Interactive Visual Analysis of Transcribed Multi-Party Discourse',\n",
       " 'The NTNU-YZU System in the AESW Shared Task: Automated Evaluation of Scientific Writing Using a Convolutional Neural Network',\n",
       " 'Empirical Studies in Learning to Read',\n",
       " 'Context-Aware In-Page Search',\n",
       " 'Combinatorial Disambiguation',\n",
       " 'Preemptive Toxic Language Detection in Wikipedia Comments Using Thread-Level Context',\n",
       " 'Medical WordNet: A New Methodology For The Construction And Validation Of Information Resources For Consumer Health',\n",
       " 'Creating a Systemic Functional Grammar Corpus from the Penn Treebank',\n",
       " 'Fluency Boost Learning and Inference for Neural Grammatical Error Correction',\n",
       " 'A Gibbs Sampler for Phrasal Synchronous Grammar Induction',\n",
       " 'Evaluating the morphological competence of Machine Translation Systems',\n",
       " 'Cross-lingual Representations with Matrix Factorization',\n",
       " 'A Semi-Supervised Approach to Improve Classification of Infrequent Discourse Relations Using Feature Vector Extension',\n",
       " 'Semantic Role Labelling with minimal resources: Experiments with French',\n",
       " 'UHH-LT at SemEval-2019 Task 6: Supervised vs. Unsupervised Transfer Learning for Offensive Language Detection.',\n",
       " 'Joint Learning with Global Inference for Comment Classification in Community Question Answering',\n",
       " 'Using Clustering to Improve Retrieval Evaluation without Relevance Judgments',\n",
       " 'Supervised classification of end-of-lines in clinical text with no manual annotation.',\n",
       " 'ConlluEditor: a fully graphical editor for Universal dependencies treebank files',\n",
       " 'Mining Parallel Corpora from Sina Weibo and Twitter',\n",
       " 'Neural Coreference Resolution with Limited Lexical Context and Explicit Mention Detection for Oral',\n",
       " '\"Not not bad\"is not\"bad\": A distributional account of negation',\n",
       " 'Factored Markov Translation with Robust Modeling',\n",
       " 'Between Reading Time and Syntactic/Semantic Categories',\n",
       " 'Robust Semantic Analysis for Unseen Data in FrameNet',\n",
       " 'Crosslingual Induction of Semantic Roles',\n",
       " 'Prototype Synthesis for Model Laws',\n",
       " 'Semantic Transliteration of Personal Names',\n",
       " 'State Gradients for RNN Memory Analysis',\n",
       " 'Learning the Scope of Hedge Cues in Biomedical Texts',\n",
       " 'Adapting taggers to Twitter with not-so-distant supervision',\n",
       " 'Feeding OWL: Extracting And Representing The Content Of Pathology Reports',\n",
       " 'Learning Field Compatibilities To Extract Database Records From Unstructured Text',\n",
       " 'A Computational Approach to the Automation of Creative Naming',\n",
       " 'Automatically Tagging Constructions of Causation and Their Slot-Fillers',\n",
       " 'Correcting Grammatical Verb Errors',\n",
       " 'Compression of Neural Machine Translation Models via Pruning',\n",
       " 'A Dynamic Oracle for Arc-Eager Dependency Parsing',\n",
       " 'Automatic generation of inter-passage links based on semantic similarity',\n",
       " 'Mining a Lexicon of Technical Terms and Lay Equivalents',\n",
       " 'A rule-based system for cross-lingual parsing of Romance languages with Universal Dependencies.',\n",
       " 'MGNC-CNN: A Simple Approach to Exploiting Multiple Word Embeddings for Sentence Classification',\n",
       " 'Multilingual Summarization Evaluation without Human Models',\n",
       " 'An Arabic Slot Grammar Parser',\n",
       " 'Large-Scale Induction and Evaluation of Lexical Resources from the Penn-II and Penn-III Treebanks',\n",
       " 'A Generalized Knowledge Hunting Framework for the Winograd Schema Challenge',\n",
       " 'Modelling Sarcasm in Twitter, a Novel Approach',\n",
       " 'Paraphrase Fragment Extraction from Monolingual Comparable Corpora',\n",
       " 'Automatic Retrieval and Clustering of Similar Words',\n",
       " '2007b. Using WYSIWYM to Create an Open-ended Interface for the Semantic Grid',\n",
       " 'Modeling Commonality Among Related Classes In Relation Extraction',\n",
       " 'Learning Phone Embeddings for Word Segmentation of Child-Directed Speech',\n",
       " 'Accelerated Estimation of Conditional Random Fields using a Pseudo-Likelihood-inspired Perceptron Variant',\n",
       " 'Normalizing Early English Letters to Present-day English Spelling',\n",
       " 'Narrative Schema as World Knowledge for Coreference Resolution',\n",
       " 'The Good, the Bad, and the Disagreement: Complex ground truth in rhetorical structure analysis',\n",
       " 'Clustering Voices in The Waste Land',\n",
       " 'Accurate Semantic Class Classifier for Coreference Resolution',\n",
       " 'Sentence Planning For Realtime Navigational Instruction',\n",
       " 'An Empirical Study on End-to-End Sentence Modelling',\n",
       " 'Ideological Perspective Detection Using Semantic Features',\n",
       " 'Localizing Moments in Video with Temporal Language',\n",
       " 'Weighted Alignment Matrices for Statistical Machine Translation',\n",
       " 'ArbEngVec : Arabic-English Cross-Lingual Word Embedding Model',\n",
       " 'Word Embeddings vs Word Types for Sequence Labeling: the Curious Case of CV Parsing',\n",
       " 'Global Learning of Noun Phrase Anaphoricity in Coreference Resolution via Label Propagation',\n",
       " 'Semantic Classification of Automatically Acquired Nouns using Lexico-Syntactic Clues',\n",
       " 'Four Techniques for Online Handling of Out-of-Vocabulary Words in Arabic-English Statistical Machine Translation',\n",
       " 'Classifying the reported ability in clinical mobility descriptions',\n",
       " 'Neural Legal Judgment Prediction in English',\n",
       " 'Automatic Extraction of Lexico-Syntactic Patterns for Detection of Negation and Speculation Scopes',\n",
       " 'Prenominal Modifier Ordering via Multiple Sequence Alignment',\n",
       " 'Bottom-Up Abstractive Summarization',\n",
       " 'RACAI’s System at PharmaCoNER 2019',\n",
       " 'Surrounding Word Sense Model for Japanese All-words Word Sense Disambiguation',\n",
       " 'Annotating Errors in Student Texts : First Experiences and Experiments',\n",
       " 'Verb Class Disambiguation Using Informative Priors',\n",
       " 'Toward Future Scenario Generation: Extracting Event Causality Exploiting Semantic Relation, Context, and Association Features',\n",
       " 'A Framework to Generate Sets of Terms from Large Scale Medical Vocabularies for Natural Language Processing',\n",
       " 'Non-Monotonic Sentence Alignment via Semisupervised Learning',\n",
       " 'Agree or Disagree: Predicting Judgments on Nuanced Assertions',\n",
       " 'Learning To Paraphrase: An Unsupervised Approach Using Multiple-Sequence Alignment',\n",
       " 'Glocal: Incorporating Global Information in Local Convolution for Keyphrase Extraction',\n",
       " 'Distinguishing Subtypes of Multiword Expressions Using Linguistically-Motivated Statistical Measures',\n",
       " 'Linguistically-Based Deep Unstructured Question Answering',\n",
       " 'The JHU Parallel Corpus Filtering Systems for WMT 2018',\n",
       " 'The Effects of Syntactic Features in Automatic Prediction of Morphology',\n",
       " 'A Corpus-Based Analysis of Geometric Constraints on Projective Prepositions',\n",
       " 'A Fine-grained Large-scale Analysis of Coreference Projection',\n",
       " 'Regular Expression Guided Entity Mention Mining from Noisy Web Data',\n",
       " 'Computational Linguistics Ensemble Methods for Native Language Identification',\n",
       " 'Combining Collocations, Lexical and Encyclopedic Knowledge for Metonymy Resolution',\n",
       " 'Reading to Learn: Constructing Features from Semantic Abstracts',\n",
       " 'Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues',\n",
       " 'Conversation Modeling on Reddit Using a Graph-Structured LSTM',\n",
       " 'Multilingual Affect Polarity and Valence Prediction in Metaphor-Rich Texts',\n",
       " 'Automatic Acquisition Of Hyponyms From Large Text Corpora',\n",
       " 'Automatic sentence classifier using sentence ordering features for Event Based Medicine: Shared task system description',\n",
       " 'Exploring Measures of â€œReadabilityâ€ for Spoken Language: Analyzing linguistic features of subtitles to identify age-specific TV programs',\n",
       " 'A Hybrid Approach to Skeleton-based Translation',\n",
       " 'Generating Discourse Inferences from Unscoped Episodic Logical Formulas',\n",
       " 'Identification of Languages in Algerian Arabic Multilingual Documents',\n",
       " 'newsLens: building and visualizing long-ranging news stories.',\n",
       " 'How Challenging is Sarcasm versus Irony Classification ? : An Analysis From Human and Computational Perspectives',\n",
       " 'Efficient Third-Order Dependency Parsers',\n",
       " 'Extending Machine Translation Evaluation Metrics with Lexical Cohesion to Document Level',\n",
       " 'Word-Based Dialect Identification with Georeferenced Rules',\n",
       " 'An Analysis of Emotion Communication Channels in Fan Fiction: Towards Emotional Storytelling',\n",
       " 'Ontology-Aware Token Embeddings for Prepositional Phrase Attachment',\n",
       " 'Implications of Pragmatic and Cognitive Theories on the Design of Utterance-Based AAC Systems',\n",
       " 'Non-textual Event Summarization by Applying Machine Learning to Template-based Language Generation',\n",
       " 'Lightly-supervised Representation Learning with Global Interpretability',\n",
       " 'Multi-modular domain-tailored OCR post-correction',\n",
       " 'Get out but don’t fall down: verb-particle constructions in child language',\n",
       " 'The Ups and Downs of Preposition Error Detection in ESL Writing',\n",
       " 'Learning Neural Templates for Text Generation',\n",
       " 'A Computationally Efficient Algorithm for Learning Topical Collocation Models',\n",
       " 'Joint A* CCG Parsing and Semantic Role Labelling',\n",
       " 'Lexical Chain Based Cohesion Models for Document-Level Statistical Machine Translation',\n",
       " 'Machine Translation with Many Manually Labeled Discourse Connectives',\n",
       " 'A Phrase Orientation Model for Hierarchical Machine Translation',\n",
       " 'Leveraging Lexical Cohesion and Disruption for Topic Segmentation',\n",
       " 'Sampling Alignment Structure under a Bayesian Translation Model',\n",
       " 'Identifying and Following Expert Investors in Stock Microblogs',\n",
       " 'A Discriminative Model for Semantics-to-String Translation',\n",
       " 'Reversibility reconsidered: finite-state factors for efficient probabilistic sampling in parsing and generation',\n",
       " 'Question Answering Using Enhanced Lexical Semantic Models',\n",
       " 'Regression and Ranking based Optimisation for Sentence Level MT Evaluation',\n",
       " 'SAMER: A Semi-Automatically Created Lexical Resource for Arabic Verbal Multiword Expressions Tokens Paradigm and their Morphosyntactic Features',\n",
       " 'Learning Sentence Ordering for Opinion Generation of Debate',\n",
       " 'Exploiting Feature Hierarchy for Transfer Learning in Named Entity Recognition',\n",
       " 'Simple Learning and Compositional Application of Perceptually Grounded Word Meanings for Incremental Reference Resolution',\n",
       " 'Elephant: Sequence Labeling for Word and Sentence Segmentation',\n",
       " 'A Graph-Based Approach to Named Entity Categorization in Wikipedia Using Conditional Random Fields',\n",
       " 'Sentiment Domain Adaptation with Multiple Sources',\n",
       " 'Sparse Information Extraction: Unsupervised Language Models to the Rescue',\n",
       " 'Correlation Coefficients and Semantic Textual Similarity',\n",
       " 'A Visualization method for machine translation evaluation results',\n",
       " 'Parsing transcripts of speech.',\n",
       " 'On the Robustness of Standalone Referring Expression Generation Algorithms Using RDF Data',\n",
       " 'Kurdish Interdialect Machine Translation',\n",
       " 'DisSent: Learning Sentence Representations from Explicit Discourse Relations',\n",
       " 'Unsupervised and Constrained Dirichlet Process Mixture Models for Verb Clustering',\n",
       " 'Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation',\n",
       " 'Exploring Named Entity Recognition As an Auxiliary Task for Slot Filling in Conversational Language Understanding',\n",
       " 'Dialectal to Standard Arabic Paraphrasing to Improve Arabic-English Statistical Machine Translation',\n",
       " 'Analysis Of Semantic Classes In Medical Text For Question Answering',\n",
       " 'Aggregation Via Set Partitioning For Natural Language Generation',\n",
       " 'Are distributional representations ready for the real world? Evaluating word vectors for grounded perceptual meaning',\n",
       " 'Language Understanding for Text-based Games Using Deep Reinforcement Learning',\n",
       " 'A Vector Space for Distributional Semantics for Entailment',\n",
       " 'Domain Adaptation with Unlabeled Data for Dialog Act Tagging',\n",
       " 'Semi-Supervised Representation Learning for Cross-Lingual Text Classification',\n",
       " 'Answering Opinion Questions with Random Walks on Graphs',\n",
       " 'Character Sequence-to-Sequence Model with Global Attention for Universal Morphological Reinflection',\n",
       " 'Using Conditional Random Fields to Extract Contexts and Answers of Questions from Online Forums',\n",
       " 'CICBUAPnlp at SemEval-2016 Task 4-A: Discovering Twitter Polarity using Enhanced Embeddings',\n",
       " 'Joint Learning from Labeled and Unlabeled Data for Information Retrieval',\n",
       " 'Disambiguated skip-gram model',\n",
       " 'Using a Wikipedia-based Semantic Relatedness Measure for Document Clustering',\n",
       " 'Domain Adaptation for CRF-based Chinese Word Segmentation using Free Annotations',\n",
       " 'Incremental Skip-gram Model with Negative Sampling',\n",
       " 'Rule-Based Named Entity Recognition in Urdu',\n",
       " 'Learning grammatical categories using paradigmatic representations: Substitute words for language acquisition',\n",
       " 'Shift-Reduce Dependency DAG Parsing',\n",
       " 'Extracting Evaluative Conditions from Online Reviews: Toward Enhancing Opinion Mining',\n",
       " 'Human Needs Categorization of Affective Events Using Labeled and Unlabeled Data',\n",
       " 'Entity Tracking Improves Cloze-style Reading Comprehension',\n",
       " 'Neural Utterance Ranking Model for Conversational Dialogue Systems',\n",
       " 'nQuery - A Natural Language Statement to SQL Query Generator',\n",
       " 'Language Identification for Creating Language-Specific Twitter Collections',\n",
       " 'Complaint Analysis and Classification for Economic and Food Safety',\n",
       " 'Modeling Missing Data in Distant Supervision for Information Extraction',\n",
       " 'Semi-Supervised Neural Machine Translation with Language Models.',\n",
       " 'Synchronous Linear Context-Free Rewriting Systems for Machine Translation',\n",
       " 'Modeling Context in Scenario Template Creation',\n",
       " 'Rational Recurrences',\n",
       " 'Finite State Transducer Calculus for Whole Word Morphology.',\n",
       " 'Dependency Parsing for Spoken Dialog Systems',\n",
       " 'Confidence Estimation for Knowledge Base Population',\n",
       " 'Interpretable Charge Predictions for Criminal Cases: Learning to Generate Court Views from Fact Descriptions',\n",
       " 'Singular Value Decomposition for Feature Selection in Taxonomy Learning',\n",
       " 'Unsupervised Aspect-Based Multi-Document Abstractive Summarization',\n",
       " 'Efficient, Feature-based, Conditional Random Field Parsing',\n",
       " 'Integrating UIMA with Alveo, a human communication science virtual laboratory',\n",
       " 'Learning Tag Embeddings and Tag-specific Composition Functions in Recursive Neural Network',\n",
       " 'CUNI System for the WMT 19 Robustness Task',\n",
       " 'An Expectation Maximization Approach To Pronoun Resolution',\n",
       " 'Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision',\n",
       " 'A Two-stage Approach for Extending Event Detection to New Types via Neural Networks',\n",
       " 'Expressing Visual Relationships via Language',\n",
       " 'ISTART: Paraphrase Recognition',\n",
       " 'Learning Generic Sentence Representations Using Convolutional Neural Networks',\n",
       " 'Automatically Generating Wikipedia Articles: A Structure-Aware Approach',\n",
       " 'Bilingual Synonym Identification with Spelling Variations',\n",
       " 'Expert, Crowdsourced, and Machine Assessment of Suicide Risk via Online Postings',\n",
       " 'Entity Disambiguation Using a Markov-Logic Network',\n",
       " 'The Use Of WordNet In Information Retrieval',\n",
       " 'Automatic Arabic Document Categorization Based On The Naive Bayes Algorithm',\n",
       " 'Wordnet-Based Cross-Language Identification of Semantic Relations',\n",
       " 'Parsing Natural Language Queries for Life Science Knowledge',\n",
       " 'Importance of Self-Attention for Sentiment Analysis',\n",
       " 'Design of a Learner Corpus for Listening and Speaking Performance',\n",
       " 'Revisiting Word Embedding for Contrasting Meaning',\n",
       " 'Combining Reinformation Learning with Information-State Update Rules',\n",
       " 'Cross-lingual and cross-domain discourse segmentation of entire documents',\n",
       " 'Learning Word Embeddings without Context Vectors.',\n",
       " 'Generative Modeling of Coordination by Factoring Parallelism and Selectional Preferences',\n",
       " 'UAlacant machine translation quality estimation at WMT 2018: a simple approach using phrase tables and feed-forward neural networks',\n",
       " 'Identifying Parallel Documents from a Large Bilingual Collection of Texts: Application to Parallel Article Extraction in Wikipedia.',\n",
       " 'Unsupervised Negation Focus Identification with Word-Topic Graph Model',\n",
       " 'Using Semantic Roles to Improve Question Answering',\n",
       " 'A Machine Learning Approach To Pronoun Resolution In Spoken Dialogue',\n",
       " 'Structural Scaffolds for Citation Intent Classification in Scientific Publications',\n",
       " 'Learning Character-level Compositionality with Visual Features',\n",
       " 'Event Detection with Hybrid Neural Architecture',\n",
       " 'Better Conversations by Modeling,Filtering,and Optimizing for Coherence and Diversity',\n",
       " 'The interpretation of non-sentential utterances in dialogue',\n",
       " 'Well-Formed Dependency to String translation with BTG Grammar',\n",
       " 'Chinese Part-Of-Speech Tagging: One-At-A-Time Or All-At-Once? Word-Based Or Character-Based?',\n",
       " 'Fixing Translation Divergences in Parallel Corpora for Neural MT',\n",
       " 'Stance Detection in Code-Mixed Hindi-English Social Media Data using Multi-Task Learning.',\n",
       " 'An Operation Sequence Model for Explainable Neural Machine Translation',\n",
       " 'Extractive vs. NLG-based Abstractive Summarization of Evaluative Text: The Effect of Corpus Controversiality',\n",
       " 'Gender Attribution: Tracing Stylometric Evidence Beyond Topic and Genre',\n",
       " 'Extraction of Domain-specific Opinion Words for Similar Domains',\n",
       " 'Integrating Collocation Features in Chinese Word Sense Disambiguation',\n",
       " 'Learning New Semi-Supervised Deep Auto-encoder Features for Statistical Machine Translation',\n",
       " 'Bilingual Product Name Dictionary Construction Using a Two Stage Method',\n",
       " 'Supervised Models for Coreference Resolution',\n",
       " 'Bilingual Experiments on an Opinion Comparable Corpus',\n",
       " 'Well-Argued Recommendation: Adaptive Models Based on Words in Recommender Systems',\n",
       " 'An Improved Graph Model for Chinese Spell Checking',\n",
       " 'Recognizing Insufficiently Supported Arguments in Argumentative Essays',\n",
       " 'Collective Opinion Target Extraction in Chinese Microblogs',\n",
       " 'Automatically Constructing a Normalisation Dictionary for Microblogs',\n",
       " 'A study of semantic augmentation of word embeddings for extractive summarization',\n",
       " 'A Kernel Independence Test for Geographical Language Variation',\n",
       " 'Automatic Extraction of English-Korean Translations for Constituents of Technical Terms',\n",
       " 'A Semantic Role-based Approach to Open-Domain Automatic Question Generation',\n",
       " 'Improving Statistical Machine Translation Performance by Oracle-BLEU Model Re-estimation',\n",
       " 'Reserating the awesometastic: An automatic extension of the WordNet taxonomy for novel terms',\n",
       " 'Chargrid: Towards Understanding 2D Documents',\n",
       " 'Cross-Lingual Dependency Parsing Using Code-Mixed TreeBank',\n",
       " 'Evaluation methods for unsupervised word embeddings',\n",
       " 'Fleshing it out: A Supervised Approach to MWE-token and MWE-type Classification',\n",
       " 'What Action Causes This? Towards Naive Physical Action-Effect Prediction',\n",
       " 'Seeing Stars: Exploiting Class Relationships For Sentiment Categorization With Respect To Rating Scales',\n",
       " 'Solving Geometry Problems: Combining Text and Diagram Interpretation',\n",
       " 'Improving Vector Space Word Representations Using Multilingual Correlation',\n",
       " 'Conundrums in Noun Phrase Coreference Resolution: Making Sense of the State-of-the-Art',\n",
       " 'Bag of Experts Architectures for Model Reuse in Conversational Language Understanding',\n",
       " 'Unsupervised Relation Extraction with General Domain Knowledge',\n",
       " 'Do Massively Pretrained Language Models Make Better Storytellers?',\n",
       " 'Multilingual Entity-Centered Sentiment Analysis Evaluated by Parallel Corpora',\n",
       " 'Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems',\n",
       " 'A preliminary study on similarity-preserving digital book identifiers',\n",
       " 'Natural Language Question Answering and Analytics for Diverse and Interlinked Datasets',\n",
       " 'Systematic Study of Long Tail Phenomena in Entity Linking',\n",
       " 'Same Referent, Different Words: Unsupervised Mining of Opaque Coreferent Mentions',\n",
       " 'Low-Resource Name Tagging Learned with Weakly Labeled Data',\n",
       " 'Bilingual Learning of Multi-sense Embeddings with Discrete Autoencoders',\n",
       " 'Statistical Phrase-Based Models For Interactive Computer-Assisted Translation',\n",
       " 'Learning Global Features for Coreference Resolution',\n",
       " 'On the Linearity of Semantic Change: Investigating Meaning Variation via Dynamic Graph Models',\n",
       " 'Learning how to Active Learn: A Deep Reinforcement Learning Approach',\n",
       " 'A global model for joint lemmatization and part-of-speech prediction',\n",
       " 'Contextual Preferences',\n",
       " 'CAT: Credibility Analysis of Arabic Content on Twitter',\n",
       " 'Learning to Compose Effective Strategies from a Library of Dialogue Components',\n",
       " 'Is Something Better than Nothing? Automatically Predicting Stance-based Arguments Using Deep Learning and Small Labelled Dataset',\n",
       " 'English-Chinese Personal Name Transliteration by Syllable-Based Maximum Matching',\n",
       " 'Understanding and Quantifying Creativity in Lexical Composition',\n",
       " 'Improving Text-to-SQL Evaluation Methodology',\n",
       " 'Improved Language Modeling by Decoding the Past',\n",
       " 'Document-level Sentiment Inference with Social, Faction, and Discourse Context',\n",
       " 'Coarse-to-Fine Syntactic Machine Translation using Language Projections',\n",
       " 'Detecting Code-Switching in a Multilingual Alpine Heritage Corpus',\n",
       " 'Unified Dependency Parsing of Chinese Morphological and Syntactic Structures',\n",
       " 'Unsupervised Source Hierarchies for Low-Resource Neural Machine Translation',\n",
       " 'A Utility Model of Authors in the Scientific Community',\n",
       " 'Sense Clustering Using Wikipedia',\n",
       " 'Phrase-Based Evaluation for Machine Translation',\n",
       " 'Improving Moderation of Online Discussions via Interpretable Neural Models',\n",
       " 'Semantic Parsing for Task Oriented Dialog using Hierarchical Representations',\n",
       " 'Temporal Information Extraction for Question Answering Using Syntactic Dependencies in an LSTM-based Architecture',\n",
       " 'Enabling Monolingual Translators: Post-Editing vs. Options',\n",
       " 'Mathematical Information Retrieval based on Type Embeddings and Query Expansion',\n",
       " 'Hallucinating Phrase Translations for Low Resource MT',\n",
       " 'A Stochastic Parser Based On An SLM With Arboreal Context Trees',\n",
       " 'Computing Lexical Contrast',\n",
       " 'MāOri Loanwords: A Corpus of New Zealand English Tweets',\n",
       " 'Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive Token Encodings',\n",
       " 'The Form Is The Substance: Classification Of Genres In Text',\n",
       " 'Improving Named Entity Recognition by Jointly Learning to Disambiguate Morphological Tags',\n",
       " 'Multi-View AdaBoost for Multilingual Subjectivity Analysis',\n",
       " 'Grammatical Error Detection Using Error- and Grammaticality-Specific Word Embeddings',\n",
       " 'Enriching Patent Search with External Keywords: a Feasibility Study',\n",
       " 'Corpora and Processing Tools for Non-Standard Contemporary and Diachronic Balkan Slavic',\n",
       " 'WARP-Text: a Web-Based Tool for Annotating Relationships between Pairs of Texts.',\n",
       " 'Visual Referring Expression Recognition: What Do Systems Actually Learn?',\n",
       " 'Certified Robustness to Adversarial Word Substitutions',\n",
       " 'Partially Specified Signatures: A Vehicle For Grammar Modularity',\n",
       " 'Stacking for Statistical Machine Translation',\n",
       " 'Chinese Informal Word Normalization: an Experimental Study',\n",
       " 'A Term Recognition Approach To Acronym Recognition',\n",
       " 'Asymmetric Features Of Human Generated Translation',\n",
       " 'Solving and Generating Chinese Character Riddles',\n",
       " 'ARS_NITK at MEDIQA 2019:Analysing Various Methods for Natural Language Inference, Recognising Question Entailment and Medical Question Answering System',\n",
       " 'Semi-supervised and unsupervised categorization of posts in Web discussion forums using part-of-speech information and minimal features',\n",
       " 'Iterative Document Representation Learning Towards Summarization with Polishing',\n",
       " 'Recognizing Biomedical Named Entities Using Skip-Chain Conditional Random Fields',\n",
       " 'Automatic Adaptation of Annotation Standards: Chinese Word Segmentation and POS Tagging – A Case Study',\n",
       " 'Findings of the WMT 2016 Bilingual Document Alignment Shared Task',\n",
       " 'Automatic Single-Document Key Fact Extraction from Newswire Articles',\n",
       " 'Self Syntactico-Semantic Enrichment of LMF Normalized Dictionaries',\n",
       " 'lex4all: A language-independent tool for building and evaluating pronunciation lexicons for small-vocabulary speech recognition',\n",
       " 'IJCNLP-2017 Task 3: Review Opinion Diversification (RevOpiD-2017)',\n",
       " 'Code Mixing: A Challenge for Language Identification in the Language of Social Media',\n",
       " 'Lexi: A tool for adaptive, personalized text simplification',\n",
       " 'Twitter Language Identification Of Similar Languages And Dialects Without Ground Truth',\n",
       " 'Annotating Claims in the Vaccination Debate',\n",
       " 'Deep Exhaustive Model for Nested Named Entity Recognition',\n",
       " 'A Hybrid Morpheme-Word Representation for Machine Translation of Morphologically Rich Languages',\n",
       " 'Think Visually: Question Answering through Virtual Imagery',\n",
       " 'Content Explorer: Recommending Novel Entities for a Document Writer',\n",
       " 'At a Glance: The Impact of Gaze Aggregation Views on Syntactic Tagging',\n",
       " 'Training a Neural Network in a Low-Resource Setting on Automatically Annotated Noisy Data',\n",
       " 'A Cascaded Approach for Social Media Text Normalization of Turkish',\n",
       " 'Term-list Translation using Mono-lingual Word Co-occurrence Vectors',\n",
       " 'Unsupervised Pretraining for Sequence to Sequence Learning',\n",
       " 'Justifying Recommendations using Distantly-Labeled Reviews and Fined-Grained Aspects',\n",
       " 'Sentence Processing in a Vectorial Model of Working Memory',\n",
       " 'Combining CNNs and Pattern Matching for Question Interpretation in a Virtual Patient Dialogue System',\n",
       " 'Toward a Task of Feedback Comment Generation for Writing Learning',\n",
       " 'Generalized Expectation Criteria for Semi-Supervised Learning of Conditional Random Fields',\n",
       " 'Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation',\n",
       " 'A Random Forest System Combination Approach for Error Detection in Digital Dictionaries',\n",
       " 'Genre Identification and the Compositional Effect of Genre in Literature',\n",
       " 'Named Entity Recognition in the Medical Domain with Constrained CRF Models',\n",
       " 'Terminology and Knowledge Representation. Italian Linguistic Resources for the Archaeological Domain',\n",
       " 'An Empirical Study on Sentiment Classification of Chinese Review using Word Embedding',\n",
       " 'Political News Sentiment Analysis for Under-resourced Languages',\n",
       " 'Sequence Tagging for Verb Conjugation in Romanian',\n",
       " 'Sentiment Composition Using a Parabolic Model',\n",
       " 'Structural Representations for Learning Relations between Pairs of Texts',\n",
       " 'KGEval: Accuracy Estimation of Automatically Constructed Knowledge Graphs',\n",
       " 'Bootstrapping Semantic Role Labelers from Parallel Data',\n",
       " 'Negation Focus Identification with Contextual Discourse Information',\n",
       " 'Bundled Gap Filling: A New Paradigm for Unambiguous Cloze Exercises',\n",
       " 'How Predictable is Your State? Leveraging Lexical and Contextual Information for Predicting Legislative Floor Action at the State Level',\n",
       " 'Generalizing Dependency Features for Opinion Mining',\n",
       " 'Investigating Different Syntactic Context Types and Context Representations for Learning Word Embeddings',\n",
       " 'Flambé: A Customizable Framework for Machine Learning Experiments.',\n",
       " 'Automatic Evaluation of Topic Coherence',\n",
       " 'Joint prediction in MST-style discourse parsing for argumentation mining',\n",
       " 'Exploiting Named Entity Taggers In A Second Language',\n",
       " 'Out-of-Domain Detection for Low-Resource Text Classification Tasks',\n",
       " 'Towards Accurate and Efficient Chinese Part-of-Speech Tagging',\n",
       " 'Classifier for Arabic Fine-Grained Dialect Identification',\n",
       " 'Character-Based Pivot Translation for Under-Resourced Languages and Domains',\n",
       " 'Semantic Structural Evaluation for Text Simplification',\n",
       " '“Was It Good? It Was Provocative.” Learning the Meaning of Scalar Adjectives',\n",
       " 'All Words Domain Adapted WSD: Finding a Middle Ground between Supervision and Unsupervision',\n",
       " 'What to talk about and how? Selective Generation using LSTMs with Coarse-to-Fine Alignment',\n",
       " 'Automatically Learning Qualia Structures From The Web',\n",
       " 'Induction of Root and Pattern Lexicon for Unsupervised Morphological Analysis of Arabic',\n",
       " 'Inducing Script Structure from Crowdsourced Event Descriptions via Semi-Supervised Clustering',\n",
       " 'Discriminative Learning of Selectional Preference from Unlabeled Text',\n",
       " 'Word Relatives in Context for Word Sense Disambiguation',\n",
       " 'Top-Rank Enhanced Listwise Optimization for Statistical Machine Translation',\n",
       " 'A Neural Approach to Automated Essay Scoring',\n",
       " 'Identifying Types of Claims in Online Customer Reviews',\n",
       " 'Better Summarization Evaluation with Word Embeddings for ROUGE',\n",
       " \"Don't Until the Final Verb Wait: Reinforcement Learning for Simultaneous Machine Translation\",\n",
       " 'Improving Word Sense Disambiguation with Linguistic Knowledge from a Sense Annotated Treebank',\n",
       " 'Analysis and Enhancement of Wikification for Microblogs with Context Expansion',\n",
       " 'Learning the Peculiar Value of Actions',\n",
       " 'Morphological Processing for English-Tamil Statistical Machine Translation',\n",
       " 'Subjectivity Recognition on Word Senses via Semi-supervised Mincuts',\n",
       " 'Recycling Terms Into A Partial Parser',\n",
       " 'CM-Net: A Novel Collaborative Memory Network for Spoken Language Understanding',\n",
       " 'Parsing English into Abstract Meaning Representation Using Syntax-Based Machine Translation',\n",
       " 'Inferring Parts Of Speech For Lexical Mappings Via The Cyc KB',\n",
       " 'Question Answering on Freebase via Relation Extraction and Textual Evidence',\n",
       " 'Improving Accuracy in Word Class Tagging through the Combination of Machine Learning Systems',\n",
       " 'Estimating User Interest from Open-Domain Dialogue',\n",
       " 'An Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation',\n",
       " 'Penalized Expectation Propagation for Graphical Models over Strings',\n",
       " 'Zero-Shot Dialog Generation with Cross-Domain Latent Actions',\n",
       " 'Abstractive Multi-document Summarization with Semantic Information Extraction',\n",
       " 'Relation Induction in Word Embeddings Revisited',\n",
       " 'Dialogue Act Modeling for Automatic Tagging and Recognition of Conversational Speech',\n",
       " 'Response Generation In Collaborative Negotiation',\n",
       " 'Identifying and Analyzing Brazilian Portuguese Complex Predicates',\n",
       " 'Mixed Language and Code-Switching in the Canadian Hansard',\n",
       " 'PaperRobot: Incremental Draft Generation of Scientific Ideas',\n",
       " 'Between Reading Time and Information Structure',\n",
       " 'Discourse-aware Statistical Machine Translation as a Context-sensitive Spell Checker',\n",
       " 'LAMB: A Good Shepherd of Morphologically Rich Languages',\n",
       " 'Semantic Changes in Japanese',\n",
       " 'Harnessing Popularity in Social Media for Extractive Summarization of Online Conversations',\n",
       " 'Integrating Subject, Type, and Property Identification for Simple Question Answering over Knowledge Base',\n",
       " 'On Using Twitter to Monitor Political Sentiment and Predict Election Results',\n",
       " 'Adapting to All Domains at Once: Rewarding Domain Invariance in SMT',\n",
       " 'Automatic Prediction of Friendship via Multi-model Dyadic Features',\n",
       " 'A Text Understander that Learns',\n",
       " 'Natural Language Programming Using Class Sequential Rules',\n",
       " 'Seeing through Deception: A Computational Approach to Deceit Detection in Written Communication',\n",
       " 'Training Word Sense Embeddings With Lexicon-based Regularization',\n",
       " 'Improving Claim Stance Classification with Lexical Knowledge Expansion and Context Utilization',\n",
       " 'Label-aware Double Transfer Learning for Cross-Specialty Medical Named Entity Recognition',\n",
       " 'Multilingual Spectral Clustering Using Document Similarity Propagation',\n",
       " 'Unsupervised Entity Linking with Abstract Meaning Representation',\n",
       " 'Latent Domain Phrase-based Models for Adaptation',\n",
       " 'Statistical Natural Language Generation from Tabular Non-textual Data',\n",
       " 'Effective Greedy Inference for Graph-based Non-Projective Dependency Parsing',\n",
       " 'Collective Annotation of Linguistic Resources: Basic Principles and a Formal Model',\n",
       " 'Unsupervised Discrete Sentence Representation Learning for Interpretable Neural Dialog Generation',\n",
       " 'SoPa: Bridging CNNs, RNNs, and Weighted Finite-State Machines',\n",
       " 'A State-of-the-Art Mention-Pair Model for Coreference Resolution',\n",
       " 'From Chinese Word Segmentation to Extraction of Constructions: Two Sides of the Same Algorithmic Coin.',\n",
       " 'Stylistic Transfer in Natural Language Generation Systems Using Recurrent Neural Networks',\n",
       " 'Zero-shot Relation Classification as Textual Entailment',\n",
       " 'Utilizing Co-Occurrence Of Answers In Question Answering',\n",
       " 'Sub-sentencial Paraphrasing by Contextual Pivot Translation',\n",
       " 'Choosing the Right Translation: A Syntactically Informed Classification Approach',\n",
       " 'A Graph-to-Sequence Model for AMR-to-Text Generation',\n",
       " 'SemEval-2019 Task 8: Fact Checking in Community Question Answering Forums',\n",
       " 'Privacy-Aware Text Rewriting.',\n",
       " 'Learning to Embed Semantic Correspondence for Natural Language Understanding',\n",
       " 'Experiments on Summary-based Opinion Classification',\n",
       " 'Metaphor Detection with Cross-Lingual Model Transfer',\n",
       " 'Text Generation From Keywords',\n",
       " 'A Dataset for Multi-Target Stance Detection',\n",
       " 'RevUP: Automatic Gap-Fill Question Generation from Educational Texts',\n",
       " 'Accurate Word Segmentation using Transliteration and Language Model Projection',\n",
       " 'Enhancing Language Models in Statistical Machine Translation with Backward N-grams and Mutual Information Triggers',\n",
       " 'A Joint Model of Rhetorical Discourse Structure and Summarization',\n",
       " 'Word Translation Disambiguation Using Bilingual Bootstrapping',\n",
       " 'Unsupervised Coreference Resolution by Utilizing the Most Informative Relations',\n",
       " 'Automatically Extracting Nominal Mentions Of Events With A Bootstrapped Probabilistic Classifier',\n",
       " 'Bootstrapping Both Product Features and Opinion Words from Chinese Customer Reviews with Cross-Inducing',\n",
       " 'Disambiguating Sentiment: An Ensemble of Humour, Sarcasm, and Hate Speech Features for Sentiment Classification',\n",
       " 'Measuring Semantic Relations between Human Activities',\n",
       " 'Identifying Offensive Posts and Targeted Offense from Twitter',\n",
       " 'Annotating the Interaction between Focus and Modality: the case of exclusive particles',\n",
       " 'SpRL-CWW: Spatial Relation Classification with Independent Multi-class Models',\n",
       " 'Learning Arguments and Supertypes of Semantic Relations Using Recursive Patterns',\n",
       " 'Predicting Success in Goal-Driven Human-Human Dialogues',\n",
       " 'Transfer in Deep Reinforcement Learning using Knowledge Graphs',\n",
       " 'Neural Ranking Models for Temporal Dependency Structure Parsing',\n",
       " 'Making Tree Kernels Practical For Natural Language Learning',\n",
       " 'Weakly Supervised Training of Semantic Parsers',\n",
       " 'Augmenting a German Morphological Database by Data-Intense Methods',\n",
       " 'Adaptive String Distance Measures for Bilingual Dialect Lexicon Induction',\n",
       " 'JUSTDeep at NLP4IF 2019 Shared Task: Propaganda Detection using Ensemble Deep Learning Models',\n",
       " 'An Unsupervised Model for Joint Phrase Alignment and Extraction',\n",
       " 'Interfacing the Lexicon and the Ontology in a Semantic Analyzer',\n",
       " 'Complex Event Extraction using DRUM',\n",
       " 'A Grounded Unsupervised Universal Part-of-Speech Tagger for Low-Resource Languages',\n",
       " 'Semi-Supervised Anaphora Resolution In Biomedical Texts',\n",
       " 'AMR-to-text generation as a Traveling Salesman Problem',\n",
       " 'Personalized Questions, Answers and Grammars: Aiding the Search for Relevant Web Information',\n",
       " 'Unsupervised Recognition of Literal and Non-Literal Use of Idiomatic Expressions',\n",
       " 'Affordance Extraction and Inference based on Semantic Role Labeling',\n",
       " 'Utilizing Variability of Time and Term Content, within and across Users in Session Detection',\n",
       " 'A Keyword-based Monolingual Sentence Aligner in Text Simplification',\n",
       " 'CaLcs: Continuously Approximating Longest Common Subsequence for Sequence Level Optimization',\n",
       " 'Towards Semantic Validation of a Derivational Lexicon',\n",
       " 'Translation Rules with Right-Hand Side Lattices',\n",
       " 'Experiments with DBpedia, WordNet and SentiWordNet as resources for sentiment analysis in micro-blogging',\n",
       " 'Neural Domain Adaptation for Biomedical Question Answering',\n",
       " 'Automatic Matching of ICD-10 codes to Diagnoses in Discharge Letters',\n",
       " 'A Best-First Anagram Hashing Filter for Approximate String Matching with Generalized Edit Distance',\n",
       " 'Demographic Classification Using Deep Multi-modal Multitask Learning',\n",
       " 'Mining the Sentiment Expectation of Nouns Using Bootstrapping Method',\n",
       " 'From newspaper to microblogging: What does it take to find opinions?',\n",
       " 'USAAR-SHEFFIELD: Semantic Textual Similarity with Deep Regression and Machine Translation Evaluation Metrics',\n",
       " 'Fluent Translations from Disfluent Speech in End-to-End Speech Translation',\n",
       " 'Exploiting Salient Patterns for Question Detection and Question Retrieval in Community-based Question Answering',\n",
       " 'Generative Content Models For Structural Analysis Of Medical Abstracts',\n",
       " 'Entity-Centric Coreference Resolution with Model Stacking',\n",
       " 'Using exemplar responses for training and evaluating automated speech scoring systems',\n",
       " 'Incorporating Dependency Trees Improve Identification of Pregnant Women on Social Media Platforms',\n",
       " 'Robust Measurement and Comparison of Context Similarity for Finding Translation Pairs',\n",
       " 'Sequential Clustering and Contextual Importance Measures for Incremental Update Summarization',\n",
       " 'Together we stand: Siamese Networks for Similar Question Retrieval',\n",
       " 'Clinical Information Retrieval using Document and PICO Structure',\n",
       " 'Exploiting Semantic Role Resources for Preposition Disambiguation',\n",
       " 'Leveraging Entity Linking and Related Language Projection to Improve Name Transliteration',\n",
       " 'A Language-Independent Approach to Automatic Text Difficulty Assessment for Second-Language Learners',\n",
       " 'Class-Driven Attribute Extraction',\n",
       " 'Ambiguity Resolution for Vt-N Structures in Chinese',\n",
       " 'Using time series and natural language processing to identify viral moments in the 2016',\n",
       " 'Do Characters Abuse More Than Words?',\n",
       " 'Disambiguating Toponyms In News',\n",
       " 'Content Models for Survey Generation: A Factoid-Based Evaluation',\n",
       " 'Using Wikipedia for Hierarchical Finer Categorization of Named Entities',\n",
       " 'The Trouble with SMT Consistency',\n",
       " 'Event Detection with Trigger-Aware Lattice Neural Network',\n",
       " 'Automatic Prediction of Aesthetics and Interestingness of Text Passages',\n",
       " 'Using LTAG-Based Features For Semantic Role Labeling',\n",
       " 'Context-aware Learning for Sentence-level Sentiment Analysis with Posterior Regularization',\n",
       " 'Evaluating Pronominal Anaphora in Machine Translation: An Evaluation Measure and a Test Suite',\n",
       " 'A Framework for Identity Resolution and Merging for Multi-source Information Extraction',\n",
       " 'Coreference Annotator A new annotation tool for aligned bilingual corpora',\n",
       " 'Chinese Syntactic Reordering for Statistical Machine Translation',\n",
       " 'Gene--disease association extraction by text mining and network analysis',\n",
       " 'Understanding the Value of Features for Coreference Resolution',\n",
       " 'Weeding out Conventionalized Metaphors: A Corpus of Novel Metaphor Annotations',\n",
       " 'Liars and Saviors in a Sentiment Annotated Corpus of Comments to Political Debates',\n",
       " 'Corpus-based Semantic Lexicon Induction with Web-based Corroboration',\n",
       " 'Using Word Embeddings for Improving Statistical Machine Translation of Phrasal Verbs',\n",
       " 'Modeling language constructs with compatibility intervals',\n",
       " 'Paraphrasing Revisited with Neural Machine Translation',\n",
       " 'Multi-Element Long Distance Dependencies: Using SPk Languages to Explore the Characteristics of Long-Distance Dependencies',\n",
       " 'Parameter Optimization for Statistical Machine Translation: It Pays to Learn from Hard Examples',\n",
       " 'What It Takes to Achieve 100% Condition Accuracy on WikiSQL',\n",
       " 'Effects of Creativity and Cluster Tightness on Short Text Clustering Performance',\n",
       " 'A Critical Reassessment of Evaluation Baselines for Speech Summarization',\n",
       " 'CFO: A Framework for Building Production NLP Systems',\n",
       " 'A Fast and Accurate Method for Approximate String Search',\n",
       " 'Semi-Automatic Identification of Bilingual Synonymous Technical Terms from Phrase Tables and Parallel Patent Sentences',\n",
       " 'Natural Language Generation from Pictographs',\n",
       " 'Annotation and Classification of French Feedback Communicative Functions',\n",
       " 'Semi-supervised sequence tagging with bidirectional language models',\n",
       " 'Holistic Sentiment Analysis Across Languages: Multilingual Supervised Latent Dirichlet Allocation',\n",
       " 'Dialog Intent Induction with Deep Multi-View Clustering',\n",
       " 'Towards Generating Personalized Hospitalization Summaries',\n",
       " 'An Evaluation of Information Extraction Tools for Identifying Health Claims in News Headlines.',\n",
       " 'Machine Comprehension by Text-to-Text Neural Question Generation',\n",
       " 'A Discriminative Topic Model using Document Network Structure',\n",
       " 'Leveraging Small Multilingual Corpora for SMT Using Many Pivot Languages',\n",
       " 'A Joint Information Model for N-Best Ranking',\n",
       " 'Context-Sensitive Spelling Correction and Rich Morphology',\n",
       " 'Aligning Predicate Argument Structures in Monolingual Comparable Texts: A New Corpus for a New Task',\n",
       " 'An Empirical Study of Translation Rule Extraction with Multiple Parsers',\n",
       " 'A Supervised Semantic Parsing with Lexicon Extension and Syntactic Constraint',\n",
       " 'Situated Question Answering In The Clinical Domain: Selecting The Best Drug Treatment For Diseases',\n",
       " 'ARAML: A Stable Adversarial Training Framework for Text Generation',\n",
       " 'Iterative Transformation of Annotation Guidelines for Constituency Parsing',\n",
       " 'Incremental Bayesian Learning of Semantic Categories',\n",
       " 'Multi-Language Surface Realisation as REST API based NLG Microservice',\n",
       " 'Unsupervised Discovery of Domain-Specific Knowledge from Text',\n",
       " 'A Platform Agnostic Dual-Strand Hate Speech Detector',\n",
       " 'StruAP: A Tool for Bundling Linguistic Trees through Structure-based Abstract Pattern',\n",
       " 'Cross-lingual Dependency Parsing with Unlabeled Auxiliary Languages',\n",
       " 'Generating Long and Informative Reviews with Aspect-Aware Coarse-to-Fine Decoding',\n",
       " 'Relation Extraction: Perspective from Convolutional Neural Networks',\n",
       " 'Measuring Topic Coherence through Optimal Word Buckets',\n",
       " 'Named Entity Recognition with Partially Annotated Training Data',\n",
       " 'Building and Refining Rhetorical-Semantic Relation Models',\n",
       " 'Assessing the Impact of Translation Errors on Machine Translation Quality with Mixed-effects Models',\n",
       " 'Flexible and Creative Chinese Poetry Generation Using Neural Memory',\n",
       " 'Lexical Substitution for the Medical Domain',\n",
       " 'Ensuring Readability and Data-fidelity using Head-modifier Templates in Deep Type Description Generation',\n",
       " 'Lexical Event Ordering with an Edge-Factored Model',\n",
       " 'Generating Tailored, Comparative Descriptions with Contextually Appropriate Intonation',\n",
       " 'Automatic conversion of sentence-end expressions for utterance characterization of dialogue systems',\n",
       " 'Expected F-Measure Training for Shift-Reduce Parsing with Recurrent Neural Networks',\n",
       " 'Composition Of Conditional Random Fields For Transfer Learning',\n",
       " 'Consistent Translation using Discriminative Learning - A Translation Memory-inspired Approach',\n",
       " 'Syntactic Constraints on Paraphrases Extracted from Parallel Corpora',\n",
       " 'Hybrid Code Networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning',\n",
       " 'A Human Judgement Corpus and a Metric for Arabic MT Evaluation',\n",
       " 'A New Corpus and Imitation Learning Framework for Context-Dependent Semantic Parsing',\n",
       " 'Semi-Supervised Semantic Role Labeling via Structural Alignment',\n",
       " 'FireCite: Lightweight real-time reference string extraction from webpages',\n",
       " 'Question Answering over Knowledge Base using Factual Memory Networks',\n",
       " 'Trouble on the Horizon: Forecasting the Derailment of Online Conversations as they Develop',\n",
       " 'LTL-UDE at SemEval-2019 Task 6: BERT and Two-Vote Classification for Categorizing Offensiveness.',\n",
       " 'Markov Logic Networks for Situated Incremental Natural Language Understanding',\n",
       " 'Improving Abstraction in Text Summarization',\n",
       " 'Speeding Up Neural Machine Translation Decoding by Cube Pruning',\n",
       " 'Semantically-Aligned Equation Generation for Solving and Reasoning Math Word Problems',\n",
       " 'Collective Search for Concept Disambiguation',\n",
       " 'The SENSEI Annotated Corpus: Human Summaries of Reader Comment Conversations in On-line News',\n",
       " 'Coupled Sequence Labeling on Heterogeneous Annotations: POS Tagging as a Case Study',\n",
       " 'Generating Natural-Language Video Descriptions Using Text-Mined Knowledge',\n",
       " 'GraphIE: A Graph-Based Framework for Information Extraction',\n",
       " 'Ramble on: tracing movements of popular historical figures',\n",
       " 'Viterbi Training for PCFGs: Hardness Results and Competitiveness of Uniform Initialization',\n",
       " 'Word Sense Disambiguation Vs. Statistical Machine Translation',\n",
       " 'The Impact of Topic Bias on Quality Flaw Prediction in Wikipedia',\n",
       " 'Learning Explicit and Implicit Structures for Targeted Sentiment Analysis',\n",
       " 'Adapting Text instead of the Model: An Open Domain Approach',\n",
       " 'Learning What to Share: Leaky Multi-Task Network for Text Classification',\n",
       " 'Chinese Poetry Generation with Recurrent Neural Networks',\n",
       " 'A New Approach For English-Chinese Named Entity Alignment',\n",
       " 'Metric Learning for Graph-Based Domain Adaptation',\n",
       " 'Vocabulary Tailored Summary Generation',\n",
       " 'A generative model for unsupervised discovery of relations and argument classes from clinical texts',\n",
       " 'Neural Response Generation via GAN with an Approximate Embedding Layer',\n",
       " 'Automatic Construction of Large Readability Corpora.',\n",
       " 'Multiplicative Tree-Structured Long Short-Term Memory Networks for Semantic Representations',\n",
       " 'Disambiguating temporal-contrastive connectives for machine translation',\n",
       " 'DCU: Aspect-based Polarity Classification for SemEval Task 4',\n",
       " 'A Hybrid Approach To Natural Language Web Search',\n",
       " 'Investigating the potential of post-ordering SMT output to improve translation quality.',\n",
       " 'Predicting Responses to Microblog Posts',\n",
       " 'A Machine Translation Approach for Chinese Whole-Sentence Pinyin-to-Character Conversion',\n",
       " 'Clustering Technique in Multi-Document Personal Name Disambiguation',\n",
       " 'Imbalanced Classification Using Dictionary-based Prototypes and Hierarchical Decision Rules for Entity Sense Disambiguation',\n",
       " 'Active Learning with Rationales for Text Classification',\n",
       " 'A POMDP-based Multimodal Interaction System Using a Humanoid Robot',\n",
       " 'Predicting Conjunct Propagation and Other Extended Stanford Dependencies',\n",
       " 'Predicting Code-switching in Multilingual Communication for Immigrant Communities',\n",
       " 'Learning Biological Processes with Global Constraints',\n",
       " 'Learning Hierarchical Translation Structure with Linguistic Annotations',\n",
       " 'Measuring the Effect of Discourse Relations on Blog Summarization',\n",
       " 'This Text Has the Scent of Starbucks: A Laplacian Structured Sparsity Model for Computational Branding Analytics',\n",
       " 'CroVeWA: Crosslingual Vector-Based Writing Assistance',\n",
       " 'MultiLing 2019: Financial Narrative Summarisation',\n",
       " 'Confidence-Weighted Learning of Factored Discriminative Language Models',\n",
       " 'PLN-PUCRS at EmoInt-2017: Psycholinguistic features for emotion intensity prediction in tweets.',\n",
       " 'Aware LSTM model for Semantic Role Labeling',\n",
       " 'Whodunnit? Crime Drama as a Case for Natural Language Understanding',\n",
       " 'Measuring Closure Properties of Patent Sublanguages',\n",
       " 'Learning Dense Representations for Entity Retrieval',\n",
       " 'Hidden-Variable Models For Discriminative Reranking',\n",
       " 'Extractive email thread summarization: Can we do better than He Said She Said?',\n",
       " 'MrMep: Joint Extraction of Multiple Relations and Multiple Entity Pairs Based on Triplet Attention',\n",
       " 'Bayesian Language Modelling of German Compounds',\n",
       " 'Parsing Models for Identifying Multiword Expressions',\n",
       " 'Retrieving Collocations From Korean Text',\n",
       " 'Zero-resource Dependency Parsing: Boosting Delexicalized Cross-lingual Transfer with Linguistic Knowledge',\n",
       " 'WIQA: A dataset for\"What if...\"reasoning over procedural text',\n",
       " 'Label Embedding using Hierarchical Structure of Labels for Twitter Classification',\n",
       " 'Transfer Learning for Neural Semantic Parsing',\n",
       " 'Annotating omission in statement pairs',\n",
       " 'A Cost Sensitive Part-of-Speech Tagging: Differentiating Serious Errors from Minor Errors',\n",
       " 'Neural Cross-Lingual Named Entity Recognition with Minimal Resources',\n",
       " 'Fast and Accurate Entity Recognition with Iterated Dilated Convolutions',\n",
       " 'Joint Language and Translation Modeling with Recurrent Neural Networks',\n",
       " 'Automatic Verb Classification Based on Statistical Distributions of Argument Structure',\n",
       " 'Ethical by Design: Ethics Best Practices for Natural Language Processing',\n",
       " 'Question Detection in Spoken Conversations Using Textual Conversations',\n",
       " 'Language Model Rest Costs and Space-Efficient Storage',\n",
       " 'Generating Informative Responses with Controlled Sentence Function',\n",
       " 'Cross-Lingual Dependency Parsing with Late Decoding for Truly Low-Resource Languages',\n",
       " 'ABSTRACT An Approach to Discourse Parsing using sangati and Rhetorical Structure Theory',\n",
       " 'Translation Quality-Based Supplementary Data Selection by Incremental Update of Translation Models',\n",
       " 'Core Semantic First: A Top-down Approach for AMR Parsing',\n",
       " 'Open Information Extraction Using Wikipedia',\n",
       " 'Translation Model Adaptation by Resampling',\n",
       " 'Using Content-level Structures for Summarizing Microblog Repost Trees',\n",
       " 'Towards Temporal Scoping of Relational Facts based on Wikipedia Data',\n",
       " 'Text Generation from Knowledge Graphs with Graph Transformers',\n",
       " 'Stress Test Evaluation for Natural Language Inference',\n",
       " 'Using PU-Learning to Detect Deceptive Opinion Spam',\n",
       " 'CoCQA: Co-Training over Questions and Answers with an Application to Predicting Question Subjectivity Orientation',\n",
       " 'Lexical Acquisition through Implicit Confirmations over Multiple Dialogues',\n",
       " 'Crowdsourced Hedge Term Disambiguation',\n",
       " 'Detection of Non-Native Sentences Using Machine-Translated Training Data',\n",
       " 'Unsupervised Relation Discovery with Sense Disambiguation',\n",
       " 'Automatic Opinion Question Generation',\n",
       " 'Advancing Linguistic Features and Insights by Label-informed Feature Grouping: An Exploration in the Context of Native Language Identification',\n",
       " 'Automated Discourse Analysis of Narrations by Adolescents with Autistic Spectrum Disorder',\n",
       " 'Paraphrasing Predicates From Written Language To Spoken Language Using The Web',\n",
       " 'Unsupervised, Knowledge-Free, and Interpretable Word Sense Disambiguation',\n",
       " 'Convolutional Neural Networks vs. Convolution Kernels: Feature Engineering for Answer Sentence Reranking',\n",
       " 'Idiom-Aware Compositional Distributed Semantics',\n",
       " 'Bootstrapping a Unified Model of Lexical and Phonetic Acquisition',\n",
       " 'How do Humans Evaluate Machine Translation',\n",
       " 'A Dataset for Noun Compositionality Detection for a Slavic Language',\n",
       " 'Learning Grammar Specifications from IGT: A Case Study of Chintang',\n",
       " 'Large-scale Opinion Relation Extraction with Distantly Supervised Neural Network',\n",
       " 'Learning Distributional Token Representations from Visual Features',\n",
       " 'Generating Templates of Entity Summaries with an Entity-Aspect Model and Pattern Mining',\n",
       " 'One Size Fits Them All? A simple LSTM for non-literal token and construction-level classification',\n",
       " 'What does Attention in Neural Machine Translation Pay Attention to?',\n",
       " 'Enhanced Aspect Level Sentiment Classification with Auxiliary Memory',\n",
       " '\"I Object!\" Modeling Latent Pragmatic Effects in Courtroom Dialogues',\n",
       " 'The Use of Dependency Relation Graph to Enhance the Term Weighting in Question Retrieval',\n",
       " 'A Simple Joint Model for Improved Contextual Neural Lemmatization',\n",
       " 'Types of Aspect Terms in Aspect-Oriented Sentiment Labeling',\n",
       " 'PDTB Discourse Parsing as a Tagging Task: The Two Taggers Approach',\n",
       " 'Inverse Document Density: A Smooth Measure for Location-Dependent Term Irregularities',\n",
       " 'LangPro: Natural Language Theorem Prover',\n",
       " 'Understanding Urban Land Use through the Visualization of Points of Interest',\n",
       " 'Grammar-Driven versus Data-Driven: Which Parsing System Is More Affected by Domain Shifts?',\n",
       " 'Encouraging Consistent Translation Choices',\n",
       " 'Landmark-Based Location Belief Tracking in a Spoken Dialog System',\n",
       " 'Automatic analysis of semantic similarity in comparable text through syntactic tree matching',\n",
       " 'Protein Word Detection using Text Segmentation Techniques',\n",
       " 'The Semantic Proto-Role Linking Model',\n",
       " 'Give Me More Feedback: Annotating Argument Persuasiveness and Related Attributes in Student Essays',\n",
       " 'Dynamically Integrating Cross-Domain Translation Memory into Phrase-Based Machine Translation during Decoding',\n",
       " 'Large-scale Analysis of Counseling Conversations: An Application of Natural Language Processing to Mental Health',\n",
       " ...]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen_s2orc_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b03ac49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_s2orc_titles_embedding = model.encode(seen_s2orc_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c68b58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10581, 384)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seen_s2orc_titles_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43be12ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "citing_titles_embedding = model.encode(citing_titles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76abbabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = citing_titles_embedding.dot(seen_s2orc_titles_embedding.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fddce4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(220, 10581)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdeed78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_indices = np.argsort(similarity_scores,axis=1)[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f607d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "SciSumm2CORWA_seen = {}\n",
    "for i, (citing_title, idx) in enumerate(zip(citing_titles_list, match_indices)):\n",
    "    if similarity_scores[i][idx] >= 0.9:\n",
    "        SciSumm2CORWA_seen[citing_titles_lookup[citing_title]] = acl_title_map_lookup[seen_s2orc_titles[idx]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b1cde9ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SciSumm2CORWA_seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a59ca69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'H05-1094': '161340',\n",
       " 'N06-1037': '15660411',\n",
       " 'P04-1054': '7395989',\n",
       " 'P05-1053': '3160937',\n",
       " 'P07-1055': '1347118',\n",
       " 'W05-0602': '2046600',\n",
       " 'W05-0636': '1544330',\n",
       " 'W06-0508': '14551434',\n",
       " 'W04-1506': '847893',\n",
       " 'N12-1007': '6557769',\n",
       " 'P11-2084': '16730027',\n",
       " 'D12-1129': '16502540',\n",
       " 'P12-1048': '3707608',\n",
       " 'P14-1012': '5709441',\n",
       " 'P14-2093': '17154151',\n",
       " 'P04-1013': '1588411',\n",
       " 'P11-1086': '311696',\n",
       " 'I08-2105': '2216585',\n",
       " 'P06-1012': '2223228',\n",
       " 'P10-1155': '5951182',\n",
       " 'E06-1010': '7966094',\n",
       " 'N07-1050': '49573952',\n",
       " 'P08-1006': '5060957',\n",
       " 'P11-2121': '13313668',\n",
       " 'W05-1505': '7966094',\n",
       " 'D08-1094': '1588782',\n",
       " 'D11-1094': '3523156',\n",
       " 'P10-1097': '5540599',\n",
       " 'W11-1310': '18347073',\n",
       " 'D12-1046': '17830435',\n",
       " 'D12-1133': '1500270',\n",
       " 'P11-1141': '15126078',\n",
       " 'W10-1404': '847893',\n",
       " 'C10-1132': '885793',\n",
       " 'C10-1135': '8027785',\n",
       " 'D12-1126': '12524250',\n",
       " 'P12-1110': '10011032',\n",
       " 'D11-1039': '1140108',\n",
       " 'D11-1140': '8597719',\n",
       " 'D12-1069': '5633240',\n",
       " 'N12-1049': '627938',\n",
       " 'P12-1045': '333563',\n",
       " 'P13-1007': '6551737',\n",
       " 'P13-2009': '1954146',\n",
       " 'W12-2802': '3476280',\n",
       " 'D12-1127': '8548302',\n",
       " 'N12-1090': '1085524',\n",
       " 'P12-3012': '2087764',\n",
       " 'P13-1155': '9600472',\n",
       " 'P13-2112': '992326',\n",
       " 'D07-1015': '11896512',\n",
       " 'D10-1004': '2997001',\n",
       " 'P08-1108': '9431510',\n",
       " 'D07-1030': '1353628',\n",
       " 'D07-1091': '2330566',\n",
       " 'D07-1092': '15522110',\n",
       " 'P07-1083': '9228771',\n",
       " 'P07-1108': '3681367',\n",
       " 'W08-0406': '11442188',\n",
       " 'D12-1108': '16112861',\n",
       " 'E12-1083': '785716',\n",
       " 'P12-1002': '10217785',\n",
       " 'P13-1109': '7721579',\n",
       " 'P13-2073': '1982217',\n",
       " 'P14-2022': '5035881',\n",
       " 'W11-2138': '238445',\n",
       " 'W12-3154': '2144828',\n",
       " 'W06-2204': '2407879',\n",
       " 'W07-1712': '763701',\n",
       " 'W09-1116': '8176815',\n",
       " 'W09-2208': '16589839',\n",
       " 'W10-3504': '11274338',\n",
       " 'D09-1161': '2699046',\n",
       " 'N03-1004': '7238979',\n",
       " 'P09-1065': '2815754'}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SciSumm2CORWA_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b80e52f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"SciSumm2CORWA_seen.json\",\"w\") as f:\n",
    "#    json.dump(SciSumm2CORWA_seen, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c94faf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_texts = {}\n",
    "for file in files:\n",
    "    paper_name = os.path.split(file)[-1]\n",
    "    xml_path = file + \"/Reference_XML/\"+paper_name+\".xml\"\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    sentences = {}\n",
    "    for node in root.findall(\"S\"):\n",
    "        sentence = node.text.strip().replace(\"  \",\" \")\n",
    "        sentence = clean_sentence(sentence)\n",
    "        sid = int(node.attrib[\"sid\"])\n",
    "        sentences[sid] = {\n",
    "            \"text\": sentence,\n",
    "            \"no_stop\": remove_stop_words(sentence).lower(),\n",
    "        }\n",
    "    for abstract_node in root.findall(\"ABSTRACT\"):\n",
    "        for node in abstract_node.findall(\"S\"):\n",
    "            sentence = node.text.strip().replace(\"  \",\" \")\n",
    "            sentence = clean_sentence(sentence)\n",
    "            sid = int(node.attrib[\"sid\"])\n",
    "            sentences[sid] = {\n",
    "                \"text\": sentence,\n",
    "                \"no_stop\": remove_stop_words(sentence).lower(),\n",
    "            }\n",
    "    for section_node in root.findall(\"SECTION\"):\n",
    "        for node in section_node.findall(\"S\"):\n",
    "            sentence = node.text.strip().replace(\"  \",\" \")\n",
    "            sentence = clean_sentence(sentence)\n",
    "            sid = int(node.attrib[\"sid\"])\n",
    "            sentences[sid] = {\n",
    "                \"text\": sentence,\n",
    "                \"no_stop\": remove_stop_words(sentence).lower(),\n",
    "            }\n",
    "    full_texts[paper_name] = sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1d60ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(sentences) for name, sentences in full_texts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0962610f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[191,\n",
       " 113,\n",
       " 176,\n",
       " 200,\n",
       " 154,\n",
       " 147,\n",
       " 415,\n",
       " 194,\n",
       " 114,\n",
       " 203,\n",
       " 198,\n",
       " 142,\n",
       " 175,\n",
       " 165,\n",
       " 233,\n",
       " 111,\n",
       " 179,\n",
       " 288,\n",
       " 257,\n",
       " 149]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0212ad4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.hist(lens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a751bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir_gold = \"/home/xxl190027/scisumm-corpus/data/Test-Set-2018-Gold/Task1/\"\n",
    "gold_files = glob(base_dir_gold+\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5ff739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_annotations = {}\n",
    "for gold_file in gold_files:\n",
    "    gold_file_name = os.path.split(gold_file)[-1]\n",
    "    paper_name, annotator = gold_file_name.split(\".\")[0].split(\"_\")\n",
    "    df = pd.read_csv(gold_file)\n",
    "    this_paper = all_annotations.get(paper_name, {})\n",
    "    for i in range(len(df)):\n",
    "        citing_paper_name = df[\"Citing Article\"][i]\n",
    "        citing_text = df[\"Citation Text Clean\"][i]\n",
    "        citation_marker = str(df[\"Citation Marker\"][i])\n",
    "        this_citance = this_paper.get(df[\"Citance Number\"][i], {\n",
    "            \"Citing Article\": citing_paper_name,\n",
    "            \"Citation Text\": citing_text,\n",
    "            \"Citation Marker\": citation_marker,\n",
    "            \"CTS\": {},\n",
    "        })\n",
    "        if len(this_citance[\"Citation Marker\"]) < len(citation_marker):\n",
    "            this_citance[\"Citation Marker\"] = citation_marker\n",
    "        #print(this_citance[\"Citation Text\"] , df[\"Citation Text Clean\"][i])\n",
    "        if this_citance[\"Citing Article\"] != df[\"Citing Article\"][i]:\n",
    "            continue\n",
    "            #print(paper_name, annotator, df[\"Citance Number\"][i], this_citance[\"Citing Article\"] , df[\"Citing Article\"][i])\n",
    "        #assert(this_citance[\"Citing Article\"] == df[\"Citing Article\"][i])\n",
    "        #assert(this_citance[\"Citation Text\"] == df[\"Citation Text Clean\"][i])\n",
    "        raw_offsets = df[\"Reference Offset\"][i]\n",
    "        offsets = []\n",
    "        if type(raw_offsets)==str and raw_offsets not in {\"0\",\"???\"}:\n",
    "            raw_offsets = raw_offsets.replace(\";\",\",\")\n",
    "            for num_str in raw_offsets.split(\",\"):\n",
    "                cleaned = num_str.strip().replace(\"'\",\"\")\n",
    "                if cleaned:\n",
    "                    offsets.append(int(cleaned))\n",
    "        elif type(raw_offsets) == np.float64 and not np.isnan(raw_offsets):\n",
    "            offsets.append(int(raw_offsets))\n",
    "        elif type(raw_offsets) == np.int64:\n",
    "            offsets.append(int(raw_offsets))\n",
    "        this_citance[\"CTS\"][annotator] = offsets\n",
    "        this_paper[df[\"Citance Number\"][i]] = this_citance\n",
    "    all_annotations[paper_name] = this_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de39d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_citation_texts = {}\n",
    "for cited_paper_name, citations in all_annotations.items():\n",
    "    for citation_idx, citation in citations.items():\n",
    "        citing_paper_name = citation[\"Citing Article\"]\n",
    "        citation_text = citation[\"Citation Text\"]\n",
    "        this_paper = all_citation_texts.get(citing_paper_name,[])\n",
    "        this_paper.append(citation_text)\n",
    "        all_citation_texts[citing_paper_name] = this_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0b0227",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_citation_texts_idx_lookup = {}\n",
    "for citing_paper_name, citation_texts in all_citation_texts.items():\n",
    "    if citing_paper_name in all_citing_paper_sentences:\n",
    "        citation_text_embedding = model.encode(citation_texts)\n",
    "        citing_paper_sentences = list(all_citing_paper_sentences[citing_paper_name].values())\n",
    "        citing_paper_sentences_embedding = model.encode(citing_paper_sentences)\n",
    "        similarity_scores = citation_text_embedding.dot(citing_paper_sentences_embedding.T)\n",
    "        match_indices = np.argsort(similarity_scores,axis=1)[:,-1]\n",
    "        citation_texts_idx_lookup = {}\n",
    "        for i, (citation_text, idx) in enumerate(zip(citation_texts, match_indices)):\n",
    "            if similarity_scores[i][idx] >= 0.9:\n",
    "                citation_texts_idx_lookup[citation_text] = idx\n",
    "        all_citation_texts_idx_lookup[citing_paper_name] = citation_texts_idx_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "no_context = 0\n",
    "for cited_paper_name, citations in all_annotations.items():\n",
    "    for citation_idx, citation in citations.items():\n",
    "        citing_paper_name = citation['Citing Article']\n",
    "        citation_text = citation['Citation Text']\n",
    "        \n",
    "        contexts = {}\n",
    "        if citing_paper_name in all_citation_texts_idx_lookup:\n",
    "            if citation_text in all_citation_texts_idx_lookup[citing_paper_name]:\n",
    "                citing_text_idx = all_citation_texts_idx_lookup[citing_paper_name][citation_text]\n",
    "                for offset in [-5,-4,-3,-2,-1,1,2,3,4,5]:\n",
    "                    this_id = citing_text_idx + offset\n",
    "                    if this_id in all_citing_paper_sentences[citing_paper_name]:\n",
    "                        contexts[offset] = all_citing_paper_sentences[citing_paper_name][this_id]\n",
    "            else:\n",
    "                pass\n",
    "                #print(citing_paper_name, citation_text)\n",
    "        else:\n",
    "            pass\n",
    "            #print(citing_paper_name)\n",
    "        citation[\"context\"] = contexts\n",
    "        if len(contexts) == 0:\n",
    "            no_context += 1\n",
    "        total += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_context, total, no_context/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8990482",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = {}\n",
    "for paper, this_paper in tqdm(all_annotations.items()):\n",
    "    reference_full_text = full_texts[paper]\n",
    "    for citance_id, this_citance in this_paper.items():\n",
    "        example_id = paper + \"_\" + str(citance_id)\n",
    "        offsets = []\n",
    "        for annotator, offset in this_citance[\"CTS\"].items():\n",
    "            offsets.extend(offset)\n",
    "        offsets = set(offsets)\n",
    "        query = remove_stop_words(this_citance[\"Citation Text\"]).lower()\n",
    "        similarities = []\n",
    "        for si, sent in reference_full_text.items():\n",
    "            scores = scorer.score(query, sent[\"no_stop\"])\n",
    "            performance = (scores[\"rouge1\"].recall + scores[\"rouge2\"].recall) / 2\n",
    "            similarities.append((si, performance))\n",
    "        sorted_similarity = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "        ranked_si = [t[0] for t in sorted_similarity]\n",
    "        annotations[example_id] = {\n",
    "            \"annotation\": offsets,\n",
    "            \"rankings\": ranked_si,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a48e1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_scisumm_citation_text_dataset = {}\n",
    "for cited_paper_name, citations in all_annotations.items():\n",
    "    this_cited_paper = {}\n",
    "    for citation_idx, citation in citations.items():\n",
    "        annotated_CTS = {}\n",
    "        for annotator, cited_sent_indices in citation[\"CTS\"].items():\n",
    "            CTS = [full_texts[cited_paper_name][idx][\"text\"] for idx in cited_sent_indices]\n",
    "            annotated_CTS[annotator] = CTS\n",
    "            \n",
    "        rouge_ranked_CTS = []\n",
    "        citation_id = cited_paper_name+\"_\"+str(citation_idx)\n",
    "        for idx in annotations[citation_id][\"rankings\"]:\n",
    "            rouge_ranked_CTS.append(full_texts[cited_paper_name][idx][\"text\"])\n",
    "            \n",
    "        this_citation = {\n",
    "            'citing_paper_name': citation['Citing Article'],\n",
    "            'citation_text': citation['Citation Text'],\n",
    "            'citation_mark': citation['Citation Marker'],\n",
    "            \"context\": citation[\"context\"],\n",
    "            \"annotated_CTS\": annotated_CTS,\n",
    "            \"rouge_ranked_CTS\": rouge_ranked_CTS,\n",
    "        }\n",
    "        this_cited_paper[int(citation_idx)] = this_citation\n",
    "    cl_scisumm_citation_text_dataset[cited_paper_name] = this_cited_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b02c796",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"cl_scisumm_citation_text_dataset.json\",\"w\") as f:\n",
    "#    json.dump(cl_scisumm_citation_text_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3b5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"cl_scisumm_citation_text_dataset.json\") as f:\n",
    "    cl_scisumm_citation_text_dataset = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18db0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_scisumm_citation_text_dataset_disjoint = {}\n",
    "for cited_paper_name, citations in cl_scisumm_citation_text_dataset.items():\n",
    "    this_cited_paper = {}\n",
    "    for citation_id, citation in citations.items():\n",
    "        if citation[\"citing_paper_name\"] not in SciSumm2CORWA_seen:\n",
    "            this_cited_paper[citation_id] = citation\n",
    "    if len(this_cited_paper) > 0:\n",
    "        cl_scisumm_citation_text_dataset_disjoint[cited_paper_name] = this_cited_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "01d58e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"cl_scisumm_citation_text_dataset_disjoint.json\",\"w\") as f:\n",
    "#    json.dump(cl_scisumm_citation_text_dataset_disjoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d50235b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The model labeled \"Old\" attempts to recreate the Char97 system using the current program.',\n",
       " 'As the generative model is top down and we use a standard bottom up best first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model.',\n",
       " 'The major problem confronting the author of a generative parser is what information to use to condition the probabilities required in the model, and how to smooth the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning.',\n",
       " 'As we discuss in more detail in Section 5, several different features in the context surrounding c are useful to include in H: the label, head pre terminal and head of the parent of c (denoted as lp, tp, hp), the label of c\\'s left sibling (lb for \"before\"), and the label of the grandparent of c (la).',\n",
       " 'We created a parser based upon the maximumentropy inspired model of the last section, smoothed using standard deleted interpolation.',\n",
       " 'As noted above, the probability model uses five smoothed probability distributions, one each for Li, M,Ri,t, and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7.',\n",
       " 'As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers.',\n",
       " 'As noted in [5], that system is based upon a \"tree bank grammar\"   a grammar read directly off the training corpus.',\n",
       " 'This is as opposed to the \"Markovgrammar\" approach used in the current parser.',\n",
       " 'Also, the earlier parser uses two techniques not employed in the current parser.',\n",
       " 'First, it uses a clustering scheme on words to give the system a \"soft\" clustering of heads and sub heads.',\n",
       " 'Second, Char97 uses unsupervised learning in that the original system was run on about thirty million words of unparsed text, the output was taken as \"correct\", and statistics were collected on the resulting parses.',\n",
       " 'As already noted, Char97 first guesses the lexical head of a constituent and then, given the head, guesses the PCFG rule used to expand the constituent in question.',\n",
       " 'As already noted, our best model uses a Markov grammar approach.',\n",
       " 'As one can see in Figure 2, a firstorder Markov grammar (with all the aforementioned improvements) performs slightly worse than the equivalent tree bank grammar parser.',\n",
       " 'That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it.',\n",
       " 'A Maximum Entropy Inspired Parser *',\n",
       " 'We present a new parser for parsing down to Penn tree bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] \"standard\" sections of the Wall Street Journal treebank.',\n",
       " 'This represents a 13% decrease in error rate over the best single parser results on this corpus [9].',\n",
       " 'The major technical innovation is the use of a \"maximum entropy inspired\" model for conditioning and smoothing that let us successfully to test and combine many different conditioning events.',\n",
       " 'We present a new parser for parsing down to Penn tree bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length < 40, and 89.5% for sentences of length < 100, when trained and tested on the previously established [5,9,10,15,17] \"standard\" sections of the Wall Street Journal tree bank.',\n",
       " 'This represents a 13% decrease in error rate over the best single parser results on this corpus [9].',\n",
       " 'Following [5,10], our parser is based upon a probabilistic generative model.',\n",
       " 'That is, for all sentences s and all parses 7r, the parser assigns a probability p(s , 7r) = p(r), the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368.',\n",
       " 'The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is s. Then for any s the parser returns the parse ir that maximizes this probability.',\n",
       " 'That is, the parser implements the function arg maxrp(7r s) = arg maxirp(7r, s) = arg maxrp(w).',\n",
       " 'What fundamentally distinguishes probabilistic generative parsers is how they compute p(r), and it is to that topic we turn next.',\n",
       " 'In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree bank grammar [3] from the training corpus.',\n",
       " 'The method that gives the best results, however, uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15].',\n",
       " 'The method we use follows that of [10].',\n",
       " 'Thus we would use p(L2 I L1, M, 1, t, h, H).',\n",
       " 'Note that the As on both ends of the expansion in Expression 2 are conditioned just like any other label in the expansion.',\n",
       " 'Also, remember that H is a pla,ceholder for any other information beyond the constituent c that may be useful in assigning c a probability.',\n",
       " 'In the parser we further assume that features are chosen from certain feature schemata and that every feature is a boolean conjunction of sub features.',\n",
       " 'Now for our purposes it is useful to rewrite this as a sequence of multiplicative functions gi(a, H) for 0 < i < j: Here go(a, H) = 11Z (H) and gi(a, H) = eAi(a,H) fi(°,11).',\n",
       " 'Maximum entropy models have two benefits for a parser builder.',\n",
       " 'First, as already implicit in our discussion, factoring the probability computation into a sequence of values corresponding to various \"features\" suggests that the probability model should be easily changeable — just change the set of features used.',\n",
       " 'This point is emphasized by Ratnaparkhi in discussing his parser [17).',\n",
       " 'Second, and this is a point we have not yet mentioned, the features used in these models need have no particular independence of one another.',\n",
       " 'This is useful if one is using a loglinear model for smoothing.',\n",
       " 'As it stands, this last equation is pretty much content free.',\n",
       " 'Suppose we were, in fact, going to compute a true maximum entropy model based upon the features used in Equation 7, Ii (t,1), f2(t,1,1p), f3(t,1,lp) .',\n",
       " 'This requires finding the appropriate Ais for Equation 3, which is accomplished using an algorithm such as iterative scaling [II] in which values for the Ai are initially \"guessed\" and then modified until they converge on stable values.',\n",
       " 'Now we observe that if we were to use a maximum entropy approach but run iterative scaling zero times, we would, in fact, just have Equation 7.',\n",
       " 'The major advantage of using Equation 7 is that one can generally get away without computing the partition function Z(H).',\n",
       " 'As partition function calculation is typically the major on line computational problem for maximum entropy models, this simplifies the model significantly.',\n",
       " 'Naturally, the distributions required by Equation 7 cannot be used without smoothing.',\n",
       " \"In a pure maximum entropy model this is done by feature selection, as in Ratnaparkhi's maximum entropy parser [17].\",\n",
       " 'While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation.',\n",
       " '(Actually, we use a minor variant described in [4].)',\n",
       " 'For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information.',\n",
       " 'We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words.',\n",
       " 'We guess the preterminals of words that are not observed in the training data using statistics on capitalization, hyphenation, word endings (the last two letters), and the probability that a given pre terminal is realized using a previously unobserved word.',\n",
       " 'L and R are conditioned on three previous labels so we are using a third order Markov grammar.',\n",
       " 'In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree bank [16] with sections 2 21 for training, section 23 for testing, and section 24 for development (debugging and tuning).',\n",
       " 'Performance on the test corpus is measured using the standard measures from [5,9,10,17].',\n",
       " 'Note that the definitions of labeled precision and recall are those given in [9] and used in all of the previous work.',\n",
       " 'As noted in [5], these definitions typically give results about 0.4% higher than the more obvious ones.',\n",
       " 'The results for the new parser as well as for the previous top three individual parsers on this corpus are given in Figure 1.',\n",
       " \"Looking in particular at the precision and recall figures, the new parser's give us a 13% error reduction over the best of the previous work, Co1199 [9].\",\n",
       " 'In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach, the aspect of the parser that is most novel.',\n",
       " 'We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives.',\n",
       " 'That parser, as stated in Figure 1, achieves an average precision/recall of 87.5.',\n",
       " 'For example, the final version of our system achieves an average precision/recall of 90.1% on the test corpus but an average precision/recall of only 89.7% on the development corpus.',\n",
       " 'It makes no use of special maximum entropyinspired features (though their presence made it much easier to perform these experiments), it does not guess the pre terminal before guessing the lexical head, and it uses a tree bank grammar rather than a Markov grammar.',\n",
       " 'This parser achieves an average precision/recall of 86.2%.',\n",
       " 'Between the Old model and the Best model, Figure 2 gives precision/recall measurements for several different versions of our parser.',\n",
       " 'One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three.',\n",
       " \"In contrast, the current parser first guesses the head's pre terminal, then the head, and then the expansion.\",\n",
       " 'It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12].',\n",
       " '(For example, part ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].)',\n",
       " 'This quantity is a relatively intuitive one (as, for example, it is the quantity used in a PCFG to relate words to their pre terminals) and it seems particularly good to condition upon here since we use it, in effect, as the unsmoothed probability upon which all smoothing of p(h) is based.',\n",
       " 'The second major reason why first guessing the pre terminal makes so much difference is that it can be used when backing off the lexical head in computing the probability of the rule expansion.',\n",
       " 'As shown in Figure 2, conditioning on this information gives a 0.6% improvement.',\n",
       " 'When we do so using our maximum entropy inspired conditioning, we get another 0.45% improvement in average precision/recall, as indicated in Figure 2 on the line labeled \"MaxEnt Inspired\\'.',\n",
       " 'Note that we also tried including this information using a standard deleted interpolation model.',\n",
       " 'Including this information within a standard deleted interpolation model causes a 0.6% decrease from the results using the less conventional model.',\n",
       " 'Indeed, the resulting performance is worse than not using this information at all.',\n",
       " 'However, a second order grammar does slightly better and a third order grammar does significantly better than the tree bank parser.',\n",
       " 'We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length < 40 and 89.5% on sentences of length < 100.',\n",
       " 'This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9].',\n",
       " 'The results of [13] achieved by combining the aforementioned three best parsers also suggest that the limit on tree bank trained parsers is much higher than previously thought.',\n",
       " 'Indeed, it may be that adding this new parser to the mix may yield still higher results.',\n",
       " 'As noted above, the main methodological innovation presented here is our \"maximumentropy inspired\" model for conditioning and smoothing.',\n",
       " 'The first is the slight, but important, improvement achieved by using this model over conventional deleted interpolation, as indicated in Figure 2.',\n",
       " 'Indeed, we initiated this line of work in an attempt to create a parser that would be flexible enough to allow modifications for parsing down to more semantic levels of detail.',\n",
       " \"We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre terminal before guessing the lexical head.\",\n",
       " 'The model assigns a probability to a parse by a top down process of considering each constituent c in Ir and for each c first guessing the pre terminal of c, t(c) (t for \"tag\"), then the lexical head of c, h(c), and then the expansion of c into further constituents e(c).',\n",
       " 'Thus the probability of a parse is given by the equation where 1(c) is the label of c (e.g., whether it is a noun phrase (np), verb phrase, etc.) and H (c) is the relevant history of c — information outside c that our probability model deems important in determining the probability in question.',\n",
       " 'Much of the interesting work is determining what goes into H (c).',\n",
       " 'Whenever it is clear to which constituent we are referring we omit the (c) in, e.g., h(c).',\n",
       " 'In this notation the above equation takes the following form: Next we describe how we assign a probability to the expansion e of a constituent.',\n",
       " 'In this scheme a traditional probabilistic context free grammar (PCFG) rule can be thought of as consisting of a left hand side with a label 1(c) drawn from the non terminal symbols of our grammar, and a right hand side that is a sequence of one or more such symbols.',\n",
       " '(We assume that all terminal symbols are generated by rules of the form \"preterm word\" and we treat these as a special case.)',\n",
       " 'For us the non terminal symbols are those of the tree bank, augmented by the symbols aux and auxg, which have been assigned deterministically to certain auxiliary verbs such as \"have\" or \"having\".',\n",
       " 'For each expansion we distinguish one of the right hand side labels as the \"middle\" or \"head\" symbol M(c).',\n",
       " 'M(c) is the constituent from which the head lexical item h is obtained according to deterministic rules that pick the head of a constituent from among the heads of its children.',\n",
       " 'To the left of M is a sequence of one or more left labels Li (c) including the special termination symbol A, which indicates that there are no more symbols to the left, and similarly for the labels to the right, Ri(c).',\n",
       " 'Thus an expansion e(c) looks like: The expansion is generated by guessing first M, then in order L1 through L,„.+1 (= A), and similarly for RI through In a pure Markov PCFG we are given the left hand side label 1 and then probabilistically generate the right hand side conditioning on no information other than 1 and (possibly) previously generated pieces of the right hand side itself.',\n",
       " 'In the simplest of such models, a zeroorder Markov grammar, each label on the righthand side is generated conditioned only on / — that is, according to the distributions p(Li j1), p(M I 1), and p(Ri I 1).',\n",
       " 'More generally, one can condition on the m previously generated labels, thereby obtaining an mth order Markov grammar.',\n",
       " 'So, for example, in a second order Markov PCFG, L2 would be conditioned on L1 and M. In our complete model, of course, the probability of each label in the expansions is also conditioned on other material as specified in Equation 1, e.g., p(e t, h, H).',\n",
       " 'For example, in a second order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1, M,1, t, h, H).',\n",
       " 'In the past few years the maximum entropy, or log linear, approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1,17].',\n",
       " 'A complete review of log linear models is beyond the scope of this paper.',\n",
       " 'Rather, we concentrate on the aspects of these models that most directly influenced the model presented here.',\n",
       " 'To compute a probability in a log linear model one first defines a set of \"features\", functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input.',\n",
       " 'In our work we assume that any feature can occur at most once, so features are boolean valued: 0 if the pattern does not occur, 1 if it does.',\n",
       " \"For example, in computing the probability of the head's pre terminal t we might want a feature schema f (t, 1) that returns 1 if the observed pre terminal of c = t and the label of c = 1, and zero otherwise.\",\n",
       " 'This feature is obviously composed of two sub features, one recognizing t, the other 1.',\n",
       " 'If both return 1, then the feature returns 1.',\n",
       " 'Now consider computing a conditional probability p(a H) with a set of features h that connect a to the history H. In a log linear model the probability function takes the following form: Here the Ai are weights between negative and positive infinity that indicate the relative importance of a feature: the more relevant the feature to the value of the probability, the higher the absolute value of the associated A.',\n",
       " 'The function Z(H), called the partition function, is a normalizing constant (for fixed H), so the probabilities over all a sum to one.',\n",
       " 'The intuitive idea is that each factor gi is larger than one if the feature in question makes the probability more likely, one if the feature has no effect, and smaller than one if it makes the probability less likely.',\n",
       " 'That is, suppose we want to compute a conditional probability p(a b, c), but we are not sure that we have enough examples of the conditioning event b, c in the training corpus to ensure that the empirically obtained probability P (a I b, c) is accurate.',\n",
       " \"The traditional way to handle this is also to compute P(a b), and perhaps P(a c) as well, and take some combination of these values as one's best estimate for p(a I b, c).\",\n",
       " 'This method is known as \"deleted interpolation\" smoothing.',\n",
       " 'In max entropy models one can simply include features for all three events f1 (a, b, c), f2 (a, b), and f3(a, c) and combine them in the model according to Equation 3, or equivalently, Equation 4.',\n",
       " 'The fact that the features are very far from independent is not a concern.',\n",
       " 'Now let us note that we can get an equation of exactly the same form as Equation 4 in the following fashion: Note that the first term of the equation gives a probability based upon little conditioning information and that each subsequent term is a number from zero to positive infinity that is greater or smaller than one if the new information being considered makes the probability greater or smaller than the previous estimate.',\n",
       " 'But let us look at how it works for a particular case in our parsing scheme.',\n",
       " 'Consider the probability distribution for choosing the pre terminal for the head of a constituent.',\n",
       " 'In Equation 1 we wrote this as p(t I 1, H).',\n",
       " 'That is, we wish to compute p(t 1, lp, tp, lb, lg, hp).',\n",
       " \"We can now rewrite this in the form of Equation 5 as follows: Here we have sequentially conditioned on steadily increasing portions of c's history.\",\n",
       " 'In many cases this is clearly warranted.',\n",
       " 'For example, it does not seem to make much sense to condition on, say, hp without first conditioning on ti,.',\n",
       " 'In other cases, however, we seem to be conditioning on apples and oranges, so to speak.',\n",
       " \"For example, one can well imagine that one might want to condition on the parent's lexical head without conditioning on the left sibling, or the grandparent label.\",\n",
       " 'One way to do this is to modify the simple version shown in Equation 6 to allow this: Note the changes to the last three terms in Equation 7.',\n",
       " 'Rather than conditioning each term on the previous ones, they are now conditioned only on those aspects of the history that seem most relevant.',\n",
       " 'The hope is that by doing this we will have less difficulty with the splitting of conditioning events, and thus somewhat less difficulty with sparse data.',\n",
       " 'We make one more point on the connection of Equation 7 to a maximum entropy formulation.',\n",
       " '...',\n",
       " 'With no prior knowledge of values for the Ai one traditionally starts with Ai = 0, this being a neutral assumption that the feature has neither a positive nor negative impact on the probability in question.',\n",
       " 'With some prior knowledge, non zero values can greatly speed up this process because fewer iterations are required for convergence.',\n",
       " 'We comment on this because in our example we can substantially speed up the process by choosing values picked so that, when the maximum entropy equation is expressed in the form of Equation 4, the gi have as their initial values the values of the corresponding terms in Equation 7.',\n",
       " '(Our experience is that rather than requiring 50 or so iterations, three suffice.)',\n",
       " 'In the simple (content free) form (Equation 6), it is clear that Z(H) = 1.',\n",
       " 'In the more interesting version, Equation 7, this is not true in general, but one would not expect it to differ much from one, and we assume that as long as we are not publishing the raw probabilities (as we would be doing, for example, in publishing perplexity results) the difference from one should be unimportant.',\n",
       " 'This allows the second pass to see expansions not present in the training corpus.',\n",
       " \"The other four equations can be found in a longer version of this paper available on the author's website (www.cs.brown.eduhiec).\",\n",
       " 'Also, the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events.',\n",
       " 'This is due to the importance of this factor in parsing, as noted in, e.g., [14].',\n",
       " 'In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (OCB), and percentage of sentences with < 2 cross brackets (2CB).',\n",
       " 'Again as standard, we take separate measurements for all sentences of length < 40 and all sentences of length < 100.',\n",
       " 'However, we do not think this aspect is the sole or even the most important reason for its comparative success.',\n",
       " 'Here we list what we believe to be the most significant contributions and give some experimental results on how well the program behaves without them.',\n",
       " '(It is \"soft\" clustering in that a word can belong to more than one cluster with different weights   the weights express the probability of producing the word given that one is going to produce a word from that cluster.)',\n",
       " 'Without these enhancements Char97 performs at the 86.6% level for sentences of length < 40.',\n",
       " 'In this section we evaluate the effects of the various changes we have made by running various versions of our current program.',\n",
       " 'To avoid repeated evaluations based upon the testing corpus, here our evaluation is based upon sentences of length < 40 from the development corpus.',\n",
       " 'We note here that this corpus is somewhat more difficult than the \"official\" test corpus.',\n",
       " 'This is indicated in Figure 2, where the model labeled \"Best\" has precision of 89.8% and recall of 89.6% for an average of 89.7%, 0.4% lower than the results on the official test corpus.',\n",
       " 'This is in accord with our experience that developmentcorpus results are from 0.3% to 0.5% lower than those obtained on the test corpus.',\n",
       " 'This is consistent with the average precision/recall of 86.6% for [5] mentioned above, as the latter was on the test corpus and the former on the development corpus.',\n",
       " \"However, Collins in [10] does not stress the decision to guess the head's pre terminal first, and it might be lost on the casual reader.\",\n",
       " 'Indeed, it was lost on the present author until he went back after the fact and found it there.',\n",
       " 'In Figure 2 we show that this one factor improves performance by nearly 2%.',\n",
       " 'It may not be obvious why this should make so great a difference, since most words are effectively unambiguous.',\n",
       " 'We believe that two factors contribute to this performance gain.',\n",
       " 'The first is simply that if we first guess the pre terminal, when we go to guess the head the first thing we can condition upon is the pre terminal, i.e., we compute p(h I t).',\n",
       " 'This one \"fix\" makes slightly over a percent difference in the results.',\n",
       " 'For example, when we first guess the lexical head we can move from computing p(r I 1,1p, h) to p(r I /, t, /p, h).',\n",
       " 'So, e.g., even if the word \"conflating\" does not appear in the training corpus (and it does not), the \"ng\" ending allows our program to guess with relative security that the word has the vbg pre terminal, and thus the probability of various rule expansions can be considerable sharpened.',\n",
       " 'For example, the tree bank PCFG probability of the rule \"VP   + vbg np\" is 0.0145, whereas once we condition on the fact that the lexical head is a vbg we get a probability of 0.214.',\n",
       " 'The second modification is the explicit marking of noun and verb phrase coordination.',\n",
       " 'We have already noted the importance of conditioning on the parent label /p.',\n",
       " 'So, for example, information about an np is conditioned on the parent — e.g., an s, vp, pp, etc.',\n",
       " 'Note that when an np is part of an np coordinate structure the parent will itself be an np, and similarly for a vp.',\n",
       " 'But nps and vps can occur with np and vp parents in non coordinate structures as well.',\n",
       " 'For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3.',\n",
       " 'Note that the subordinate vp has a vp parent.',\n",
       " 'Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure.',\n",
       " 'A vp coordinate structure is defined here as a constituent with two or more vp children, one or more of the constituents comma, cc, conjp (conjunctive phrase), and nothing else; coordinate np phrases are defined similarly.',\n",
       " 'Something very much like this is done in [15].',\n",
       " \"We believe that this is mostly due to improvements in guessing the sub constituent's pre terminal and head.\",\n",
       " 'Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while.',\n",
       " 'Next we add the less obvious conditioning events noted in our previous discussion of the final model — grandparent label lg and left sibling label /b.',\n",
       " 'The results here are shown in the line \"Standard Interpolation\".',\n",
       " 'Up to this point all the models considered in this section are tree bank grammar models.',\n",
       " 'That is, the PCFG grammar rules are read directly off the training corpus.',\n",
       " 'The results reported here disprove this conjecture.',\n",
       " \"From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head's pre terminal before guessing the head.\",\n",
       " 'Neither of these results were anticipated at the start of this research.',\n",
       " 'Two aspects of this model deserve some comment.',\n",
       " 'We expect that as we experiment with other, more semantic conditioning information, the importance of this aspect of the model will increase.',\n",
       " 'More important in our eyes, though, is the flexibility of the maximum entropy inspired model.',\n",
       " 'Though in some respects not quite as flexible as true maximum entropy, it is much simpler and, in our estimation, has benefits when it comes to smoothing.',\n",
       " 'Ultimately it is this flexibility that let us try the various conditioning events, to move on to a Markov grammar approach, and to try several Markov grammars of different orders, without significant programming.',\n",
       " 'It is to this project that our future parsing work will be devoted.']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_scisumm_citation_text_dataset['A00-2018'][\"2\"][\"rouge_ranked_CTS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408bf0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_regressive=False\n",
    "context_window_size = 2\n",
    "use_annotated_CTS = False\n",
    "n_docs=10\n",
    "mask_token = \"<mask>\"\n",
    "sep_token = \"<sep>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29763bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for cited_paper_name, citations in cl_scisumm_citation_text_dataset.items():\n",
    "    for citation_idx, citation in citations.items():\n",
    "        citation_id = cited_paper_name+\"_\"+str(citation_idx)\n",
    "        source = \"\"\n",
    "        for i in range(context_window_size*-1,0,1):\n",
    "            if i in citation[\"context\"]:\n",
    "                source += citation[\"context\"][i] + \"\\n \"\n",
    "        source += \" [Dominant] \"+ mask_token + \" \"\n",
    "        if not auto_regressive:\n",
    "            for i in range(1, context_window_size+1):\n",
    "                if i in citation[\"context\"]:\n",
    "                    source += citation[\"context\"][i] + \"\\n \"\n",
    "        if use_annotated_CTS:\n",
    "            citation_marks = [citation[\"citation_mark\"]]\n",
    "            annotated_CTS = []\n",
    "            for author, CTS in citation[\"annotated_CTS\"].items():\n",
    "                annotated_CTS.extend(CTS)\n",
    "            annotated_CTS = list(set(annotated_CTS))\n",
    "            \n",
    "            for sentence in annotated_CTS:\n",
    "                source += \" \"+sep_token+\" \"+ citation[\"citation_mark\"] + \" // \" + sentence\n",
    "                \n",
    "        else:\n",
    "            citation_marks = [citation[\"citation_mark\"]]\n",
    "            for sentence in citation[\"rouge_ranked_CTS\"][:n_docs]:\n",
    "                source += \" \"+ sep_token+\" \"+ citation[\"citation_mark\"] + \" // \" + sentence\n",
    "\n",
    "        target = citation[\"citation_text\"]\n",
    "        samples.append({\n",
    "            \"id\": citation_id,\n",
    "            \"source\": source,\n",
    "            \"target\": target,\n",
    "            \"citations\": \"#\".join(citation_marks)\n",
    "        })\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e355b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a80ac5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cited_paper[\"support_sentences\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a587a335",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
